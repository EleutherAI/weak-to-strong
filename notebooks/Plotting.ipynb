{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9a4b5a",
   "metadata": {},
   "source": [
    "# Simple Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"../../your_sweep_path/default\"\n",
    "\n",
    "PLOT_ALL_SEEDS = False\n",
    "# Full sweep\n",
    "MODELS_TO_PLOT = [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"Qwen/Qwen-1_8B\", \"Qwen/Qwen-7B\", \"Qwen/Qwen-14B\"]\n",
    "# Minimal sweep\n",
    "# MODELS_TO_PLOT = [\"gpt2\", \"gpt2-medium\", \"gpt2-large\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5caa051",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for result_filename in glob.glob(os.path.join(RESULTS_PATH, \"**/results_summary.json\"), recursive=True):\n",
    "    config_file = os.path.join(\"/\".join(result_filename.split(\"/\")[:-1]), \"config.json\")\n",
    "    config = json.load(open(config_file, \"r\"))\n",
    "    if config[\"model_size\"] not in MODELS_TO_PLOT:\n",
    "        continue\n",
    "    if 'seed' not in config:\n",
    "        config['seed'] = 0\n",
    "    record = config.copy()\n",
    "    if 'weak_model' in config:\n",
    "        for k in record['weak_model']:\n",
    "            if k == 'model_size':\n",
    "                assert record['weak_model'][k] == record['weak_model_size']\n",
    "            record['weak_' + k] = record['weak_model'][k]\n",
    "        del record['weak_model']\n",
    "    record.update(json.load(open(result_filename)))\n",
    "    records.append(record)\n",
    "\n",
    "df = pd.DataFrame.from_records(records).sort_values(['ds_name', 'model_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f628577",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = df.ds_name.unique()\n",
    "for dataset in datasets:\n",
    "    cur_df = df[(df.ds_name == dataset)].copy()\n",
    "    base_accuracies = cur_df[cur_df['weak_model_size'].isna()].groupby('model_size').agg({'accuracy': 'mean', 'seed': 'count'}).sort_values('accuracy')\n",
    "    base_accuracy_lookup = base_accuracies['accuracy'].to_dict()\n",
    "    base_accuracies = base_accuracies.reset_index()\n",
    "\n",
    "    cur_df['strong_model_accuracy'] = cur_df['model_size'].apply(lambda x: base_accuracy_lookup[x])\n",
    "    cur_df.loc[~cur_df['weak_model_size'].isna(), 'weak_model_accuracy'] = cur_df.loc[~cur_df['weak_model_size'].isna(), 'weak_model_size'].apply(lambda x: base_accuracy_lookup[x])\n",
    "\n",
    "    # Exclude cases where the weak model is better than the strong model from PGR calculation.\n",
    "    valid_pgr_index = (\n",
    "        (~cur_df['weak_model_size'].isna()) & \n",
    "        (cur_df['weak_model_size'] != cur_df['model_size']) & \n",
    "        (cur_df['strong_model_accuracy'] > cur_df['weak_model_accuracy'])\n",
    "    )\n",
    "    cur_df.loc[valid_pgr_index, 'pgr'] = (cur_df.loc[valid_pgr_index, 'accuracy'] - cur_df.loc[valid_pgr_index, 'weak_model_accuracy']) / (cur_df.loc[valid_pgr_index, 'strong_model_accuracy'] - cur_df.loc[valid_pgr_index, 'weak_model_accuracy'])\n",
    "\n",
    "    cur_df.loc[cur_df['weak_model_size'].isna(), \"weak_model_size\"] = \"ground truth\"\n",
    "\n",
    "    for seed in [None] + (sorted(cur_df['seed'].unique().tolist()) if PLOT_ALL_SEEDS else []):\n",
    "        plot_df = cur_df.copy().sort_values(['strong_model_accuracy']).sort_values(['loss'], ascending=False)\n",
    "        if seed is not None:\n",
    "            plot_df = plot_df[plot_df['seed'] == seed]\n",
    "\n",
    "        print(f\"Dataset: {dataset} (seed: {seed})\")\n",
    "\n",
    "        pgr_results = plot_df[~plot_df['pgr'].isna()].groupby(['loss']).aggregate({\"pgr\": \"median\"})\n",
    "\n",
    "        palette = sns.color_palette('colorblind', n_colors=len(plot_df['weak_model_size'].unique()) - 1)\n",
    "        color_dict = {model: (\"black\" if model == 'ground truth' else palette.pop()) for model in plot_df['weak_model_size'].unique()}\n",
    "\n",
    "        sns.lineplot(data=plot_df, x='strong_model_accuracy', y='accuracy', hue='weak_model_size', style='loss', markers=True, palette=color_dict)\n",
    "        pd.plotting.table(plt.gca(), pgr_results.round(4), loc='lower right', colWidths=[0.1, 0.1], cellLoc='center', rowLoc='center')\n",
    "        plt.xticks(ticks=base_accuracies['accuracy'], labels=[f\"{e} ({base_accuracy_lookup[e]:.4f})\" for e in base_accuracies['model_size']], rotation=90)\n",
    "        plt.title(f\"Dataset: {dataset} (seed: {seed})\")\n",
    "        plt.legend(loc='upper left')\n",
    "        suffix = \"\"\n",
    "        if seed is not None:\n",
    "            suffix = f\"_{seed}\"\n",
    "        plt.savefig(f\"{dataset.replace('/', '-')}{suffix}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553e612",
   "metadata": {},
   "source": [
    "# Calibration w.r.t. the soft labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "\n",
    "# strong_path = \"../results/conf_disagree/bs=32-dn=sciq_with_supp-ge=3-lp=0-l=xent-l=4e-05-ls=cosi_anne-mc=512-ms=Qwen1.5-0.5B-ntd=500-ntd=4300-ntd=4300-o=adam-s=0-twd=0\"\n",
    "# w2s_path = \"../results/conf_disagree/bs=32-dn=sciq_with_supp-lp=0-l=kl-l=2e-05-ls=cosi_anne-mc=512-ms=Qwen1.5-0.5B-ntd=500-ntd=4300-ntd=4300-o=adam-s=0-see=15-twd=0-we=2-wlf=0.5-wms=opt-350m\"\n",
    "strong_path = \"../results/grads/bs=32-dl=1-dn=sciq_for_lm_head_with_supp-gib=1-ge=3-gee=10000000-lp=0-lbmae=0-l=xent-l=4e-05-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=1000-ntd=3000-ntd=6000-o=adam-stl=50-s=0-twd=0\"\n",
    "w2s_path = \"../results/grads/bs=32-dl=1-dn=sciq_for_lm_head_with_supp-gib=1-lp=0-lbmae=0-l=kl-l=2e-05-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=1000-ntd=3000-ntd=6000-o=adam-stl=50-s=0-twd=0-we=3-wee=25-wlf=0.5-wms=opt-350m\"\n",
    "eval_results_paths = [p for p in os.listdir(w2s_path) if p.startswith(\"eval_results\") and p[-1].isdigit()]\n",
    "eval_results_paths.sort(key=lambda x: int(x.split(\"_\")[-1]))\n",
    "\n",
    "weak_soft_labels = None\n",
    "gt_labels = None\n",
    "pred_probs = []\n",
    "for p in eval_results_paths:\n",
    "    eval_results = load_from_disk(os.path.join(w2s_path, p)).with_format(\"numpy\")\n",
    "    if weak_soft_labels is None:\n",
    "        weak_soft_labels = eval_results['weak_soft_label'][:, 1]  # type: ignore\n",
    "    else:\n",
    "        assert np.all(weak_soft_labels == eval_results['weak_soft_label'][:, 1])  # type: ignore\n",
    "    if gt_labels is None:\n",
    "        gt_labels = eval_results['soft_label'][:, 1]  # type: ignore\n",
    "    else:\n",
    "        assert np.all(gt_labels == eval_results['soft_label'][:, 1])  # type: ignore\n",
    "    pred_probs.append(eval_results['soft_pred'][:, 1])  # type: ignore\n",
    "\n",
    "weak_soft_labels = np.array(weak_soft_labels)\n",
    "pred_probs = np.array(pred_probs)\n",
    "gt_labels = np.array(gt_labels)\n",
    "weak_soft_labels.shape, pred_probs.shape, gt_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf36650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "\n",
    "n_time_steps = len(pred_probs)\n",
    "# Create base figure\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter(x=pred_probs[0], y=weak_soft_labels, mode='markers',  \n",
    "                     marker=dict(color=gt_labels, colorscale='Viridis', colorbar=dict(title='GT Label')))],\n",
    "    layout=go.Layout(\n",
    "        title=\"Soft Labels Over Time\",\n",
    "        xaxis=dict(range=[0, 1], title=\"Strong student predicted probability\"),\n",
    "        yaxis=dict(range=[0, 1], title=\"Weak supervisor soft label\"),\n",
    "        updatemenus=[dict(\n",
    "            type=\"buttons\",\n",
    "            buttons=[dict(label=\"Play\",\n",
    "                          method=\"animate\",\n",
    "                          args=[None, {\"frame\": {\"duration\": 700, \"redraw\": True}, \"fromcurrent\": True}]),\n",
    "                    dict(label=\"Pause\",\n",
    "                         method=\"animate\",\n",
    "                         args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False}, \"mode\": \"immediate\"}])\n",
    "                    ])]\n",
    "    ),\n",
    "    frames=[go.Frame(data=[go.Scatter(x=pred_probs[i], y=weak_soft_labels, mode='markers',\n",
    "                                        marker=dict(color=gt_labels, colorscale='Viridis', colorbar=dict(title='GT Label')))],\n",
    "                         name=str(i))\n",
    "            for i in range(1, n_time_steps)]\n",
    ")\n",
    "\n",
    "# Add axis titles\n",
    "fig.update_layout(xaxis_title=\"Strong student predicted probability\", yaxis_title=\"Weak supervisor soft label\", height=800, width=900)\n",
    "\n",
    "\n",
    "# Show animation\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caeef6",
   "metadata": {},
   "source": [
    "# Get gradients per example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_eval_results_path = os.path.join(strong_path, \"eval_results_final\")\n",
    "final_eval_results_path = os.path.join(w2s_path, \"eval_results_final\")\n",
    "final_eval_results = load_from_disk(final_eval_results_path)\n",
    "strong_eval_results = load_from_disk(strong_eval_results_path)\n",
    "print(len(final_eval_results), len(strong_eval_results))\n",
    "strong_eval_results = strong_eval_results.select(range(len(final_eval_results)))\n",
    "assert np.all(np.array(final_eval_results['id']) == np.array(strong_eval_results['id']))\n",
    "final_eval_results = final_eval_results.add_column(\"strong_soft_pred\", strong_eval_results['soft_pred'])  # type: ignore\n",
    "final_eval_results = final_eval_results.add_column(\"strong_hard_pred\", strong_eval_results['hard_pred'])  # type: ignore\n",
    "final_eval_results = final_eval_results.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37852215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "from weak_to_strong.datasets import load_and_process_dataset, tokenize_dataset\n",
    "from weak_to_strong.config import ModelConfig, MODELS_DICT, LOSS_DICT\n",
    "from weak_to_strong.model import TransformerWithHead\n",
    "from weak_to_strong.common import get_tokenizer, to_batch\n",
    "from weak_to_strong.train import maybe_load_model\n",
    "\n",
    "config = json.load(open(os.path.join(w2s_path, \"config.json\"), \"r\"))\n",
    "model_name = config[\"model_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_lm_head = \"choice_input_ids\" in final_eval_results.column_names\n",
    "loss_fn = LOSS_DICT[config[\"loss\"]]\n",
    "\n",
    "d_proj = 10_000\n",
    "proj_grads, hiddens = defaultdict(lambda: defaultdict(dict)), defaultdict(lambda: defaultdict(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a132b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_type, results_path in [(\"w2s\", w2s_path), (\"strong\", strong_path),]:\n",
    "\n",
    "    # init model\n",
    "    mcfg = MODELS_DICT[model_name].copy()\n",
    "    if config[\"disable_lora\"]:\n",
    "        mcfg[\"lora_modules\"] = None\n",
    "    model_config = ModelConfig(**mcfg)\n",
    "    model = TransformerWithHead.from_pretrained(  # type: ignore\n",
    "                model_config.name,\n",
    "                lora_modules=model_config.lora_modules,\n",
    "                use_lm_head=use_lm_head,\n",
    "                num_labels=2,\n",
    "                linear_probe=config[\"linear_probe\"],\n",
    "                **model_config.custom_kwargs,\n",
    "            ).cuda()\n",
    "\n",
    "    model_n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    torch.manual_seed(0)  # ensure that our projection is the same across runs and examples\n",
    "    proj_basis_indices = torch.randint(0, model_n_params, (d_proj,))\n",
    "    print(f\"Hash(projection indices): {proj_basis_indices.sum().item()}\")\n",
    "    \n",
    "    for target_label_column in [\"weak_soft_label\", \"soft_label\"]:\n",
    "        print(f\"Computing gradients for {target_label_column} in {run_type}\")\n",
    "        for checkpoint in os.listdir(results_path):\n",
    "            if not checkpoint.startswith(\"checkpoint\") or run_type in proj_grads and checkpoint in proj_grads[run_type] and target_label_column in proj_grads[run_type][checkpoint]:\n",
    "                continue\n",
    "            print(f\"Loading model from {checkpoint}\")\n",
    "            proj_grads[run_type][checkpoint][target_label_column] = torch.zeros((len(final_eval_results), d_proj), device=model.device)\n",
    "            hiddens[run_type][checkpoint][target_label_column] = torch.zeros((len(final_eval_results), model.config.hidden_size), device=model.device)\n",
    "\n",
    "            # load model checkpoint\n",
    "            assert maybe_load_model(model, os.path.join(results_path, checkpoint))\n",
    "            model.eval()\n",
    "\n",
    "            for i, batch in tqdm(enumerate(to_batch(final_eval_results, batch_size=1))):\n",
    "                input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "                            [ex for ex in batch[\"input_ids\"]], batch_first=True\n",
    "                    ).to(model.device)\n",
    "                label = batch[target_label_column].to(model.device)\n",
    "                        \n",
    "                logits, hs = model(\n",
    "                                input_ids, choice_input_ids=batch.get(\"choice_input_ids\"), output_hidden_states=True\n",
    "                    )\n",
    "                    \n",
    "                loss = loss_fn(logits, label, step_frac=0)\n",
    "                loss.backward()\n",
    "\n",
    "                # get grad in a vector\n",
    "                grad = torch.cat([p.grad.clone().detach().flatten() for p in model.parameters() if p.requires_grad])\n",
    "                # project onto random direction\n",
    "                proj_grads[run_type][checkpoint][target_label_column][i, :] = grad[proj_basis_indices]\n",
    "                # proj_grads[i] = torch.cat([p.grad.clone().detach().flatten() for p in model.score.parameters() if p.requires_grad])\n",
    "\n",
    "                hiddens[run_type][checkpoint][target_label_column][i, :] = hs[-1][0, -1, :].detach().clone().cpu()\n",
    "\n",
    "                # zero out grads\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93246537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(defdict):\n",
    "    return {k: to_dict(v) if isinstance(v, defaultdict) else v for k, v in defdict.items()}\n",
    "torch.save(to_dict(proj_grads), os.path.join(w2s_path, \"proj_grads.pt\"))\n",
    "torch.save(to_dict(hiddens), os.path.join(w2s_path, \"hiddens.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(proj_grads[\"w2s\"].keys()) - set(proj_grads[\"strong\"].keys())\n",
    "ckpt = \"checkpoint_0.bin\"\n",
    "target_label_column = \"soft_label\"\n",
    "ckpt_grads = proj_grads[\"w2s\"][ckpt][target_label_column]\n",
    "# ckpt_hiddens = hiddens[\"w2s\"][ckpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_grads_normalized = ckpt_grads / ckpt_grads.norm(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ccc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_str(txt):\n",
    "    last_quote = txt.rfind('\"')\n",
    "    penultimate_quote = txt.rfind('\"', 0, last_quote)\n",
    "    return txt[penultimate_quote + 1:last_quote]\n",
    "is_answer_in_support = np.array([s.count(get_answer_str(s)) > 1 for s in final_eval_results[\"txt\"]]) \n",
    "exists_support = np.array([not s.startswith(\"Name: Bob\\n\\nPassage 1:\\n\\n\\nQ1:\") for s in final_eval_results[\"txt\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10934cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_labels = np.array(final_eval_results[\"soft_label\"])[:, 1]\n",
    "weak_soft_labels = np.array(final_eval_results[\"weak_soft_label\"])[:, 1]\n",
    "strong_soft_preds = np.array(final_eval_results[\"strong_soft_pred\"])[:, 1]\n",
    "w2s_soft_preds = np.array(final_eval_results[\"soft_pred\"])[:, 1]\n",
    "\n",
    "weak_error = (weak_soft_labels - soft_labels)\n",
    "weak_diff_from_strong = (weak_soft_labels - strong_soft_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# project gradients into 2D\n",
    "\n",
    "use_normalized = False\n",
    "mat = proj_grads_normalized if use_normalized else ckpt_grads\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "proj_2d = pca.fit_transform(mat.cpu().numpy())\n",
    "# proj_2d = pca.transform(mat.cpu().numpy())\n",
    "print(\"explained variance\", pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(4, 3), dpi=250)\n",
    "plt.scatter(proj_2d[:, 0], proj_2d[:, 1], c=weak_error, cmap='coolwarm', alpha=0.5, marker='o', vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Gradient PC 1\")\n",
    "plt.ylabel(\"Gradient PC 2\")\n",
    "plt.title(\"Normalized gradients projected into 2D\" if use_normalized else \"Gradients projected into 2D\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c3218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
