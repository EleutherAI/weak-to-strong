{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_from_disk\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class SplitResults():\n",
    "    # this could just be a dataset but I want type safety about the columns\n",
    "    n: int\n",
    "    ids: np.ndarray  # (n,)\n",
    "    weak_soft_labels: np.ndarray  # (n,)\n",
    "    gt_soft_labels: np.ndarray  # (n,)\n",
    "    weak_hard_labels: np.ndarray  # (n,)\n",
    "    gt_hard_labels: np.ndarray  # (n,)\n",
    "    logodds: np.ndarray  # (n,)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert len(self.ids) == self.n\n",
    "        assert len(self.weak_soft_labels) == self.n\n",
    "        assert len(self.gt_soft_labels) == self.n\n",
    "        assert len(self.weak_hard_labels) == self.n\n",
    "        assert len(self.gt_hard_labels) == self.n\n",
    "        assert self.logodds.shape == (self.n,)\n",
    "\n",
    "    def truncate_to(self, n: int):\n",
    "        return SplitResults(\n",
    "            n=n,\n",
    "            ids=self.ids[:n],\n",
    "            weak_soft_labels=self.weak_soft_labels[:n],\n",
    "            gt_soft_labels=self.gt_soft_labels[:n],\n",
    "            weak_hard_labels=self.weak_hard_labels[:n],\n",
    "            gt_hard_labels=self.gt_hard_labels[:n],\n",
    "            logodds=self.logodds[:n],\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GradResults():\n",
    "    n_steps: int\n",
    "    ids: np.ndarray  # (n,)\n",
    "    steps: np.ndarray  # (n,)\n",
    "    lrs: np.ndarray  # (n,)\n",
    "    kernel_grads: np.ndarray  # (n, n_test)\n",
    "    test_logodds: np.ndarray  # (n, n_test), logodds before the update\n",
    "    proj_basis_indices: np.ndarray  # (d_down,)\n",
    "    weak_soft_labels: np.ndarray  # (n,)\n",
    "    gt_soft_labels: np.ndarray # (n,)\n",
    "    weak_error: np.ndarray = field(init=False)  # (n,)  # weak_soft_labels - gt_soft_labels\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.weak_error = self.weak_soft_labels - self.gt_soft_labels\n",
    "        assert self.ids.shape == (self.n,)\n",
    "        assert self.steps.shape == (self.n,)\n",
    "        assert self.lrs.shape == (self.n,)\n",
    "        assert self.kernel_grads.shape == (self.n, self.test_logodds.shape[1])\n",
    "        assert self.test_logodds.shape == (self.n, self.kernel_grads.shape[1])\n",
    "        assert self.weak_soft_labels.shape == (self.n,)\n",
    "        assert self.gt_soft_labels.shape == (self.n,)\n",
    "        assert self.weak_error.shape == (self.n,)\n",
    "\n",
    "    def truncate_test_to(self, n: int):\n",
    "        return GradResults(\n",
    "            n_steps=self.n_steps,\n",
    "            ids=self.ids,\n",
    "            steps=self.steps,\n",
    "            lrs=self.lrs,\n",
    "            kernel_grads=self.kernel_grads[:, :n],\n",
    "            test_logodds=self.test_logodds[:, :n],\n",
    "            proj_basis_indices=self.proj_basis_indices,\n",
    "            weak_soft_labels=self.weak_soft_labels,\n",
    "            gt_soft_labels=self.gt_soft_labels,\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class RunResult():\n",
    "    weak_acc: float\n",
    "    strong_acc: float\n",
    "    w2s_acc: float\n",
    "    pgr: float\n",
    "    grads: GradResults  # with n=n_train_steps\n",
    "    w2s_train: SplitResults  \n",
    "    w2s_vals: dict[int, SplitResults]  # step -> SplitResults\n",
    "    w2s_test: SplitResults\n",
    "    weak_test: SplitResults\n",
    "    strong_test: SplitResults\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not (self.w2s_test.n == self.weak_test.n == self.strong_test.n):\n",
    "            # sometimes these are different sizes when the runs used different batch sizes\n",
    "            min_n = min(self.w2s_test.n, self.weak_test.n, self.strong_test.n)\n",
    "            \n",
    "            # truncate to the minimum length\n",
    "            self.w2s_test = self.w2s_test.truncate_to(min_n)\n",
    "            self.weak_test = self.weak_test.truncate_to(min_n)\n",
    "            self.strong_test = self.strong_test.truncate_to(min_n)\n",
    "\n",
    "        if self.w2s_test.n != self.grads.kernel_grads.shape[1]:\n",
    "            # TODO: make it so we don't have to assume these are aligned\n",
    "            warnings.warn(f\"Test set size ({self.w2s_test.n}) does not match number of test examples in kernel_grads ({self.grads.kernel_grads.shape[1]})\")\n",
    "            self.grads = self.grads.truncate_test_to(self.w2s_test.n)\n",
    "            \n",
    "        v0 = self.w2s_vals.get(0)\n",
    "        if v0 is not None:\n",
    "            assert all(np.all(v.ids == v0.ids) for v in self.w2s_vals.values())\n",
    "            assert all(np.all(v.weak_soft_labels == v0.weak_soft_labels) for v in self.w2s_vals.values())\n",
    "            assert all(np.all(v.gt_soft_labels == v0.gt_soft_labels) for v in self.w2s_vals.values())\n",
    "            assert all(np.all(v.weak_hard_labels == v0.weak_hard_labels) for v in self.w2s_vals.values())\n",
    "            assert all(np.all(v.gt_hard_labels == v0.gt_hard_labels) for v in self.w2s_vals.values())\n",
    "\n",
    "        assert np.all(self.w2s_test.ids == self.weak_test.ids)\n",
    "        assert np.all(self.w2s_test.ids == self.strong_test.ids)\n",
    "        assert np.all(self.w2s_test.gt_soft_labels == self.weak_test.gt_soft_labels)\n",
    "        assert np.all(self.w2s_test.gt_soft_labels == self.strong_test.gt_soft_labels)\n",
    "        assert np.all(self.w2s_test.gt_hard_labels == self.weak_test.gt_hard_labels)\n",
    "        assert np.all(self.w2s_test.gt_hard_labels == self.strong_test.gt_hard_labels)\n",
    "        assert self.w2s_test.n == self.grads.kernel_grads.shape[1]\n",
    "\n",
    "\n",
    "def load_run_result(w2s_path: str, weak_path: str, strong_path: str) -> RunResult:\n",
    "        \n",
    "    ### Gather validation results from throughout training ###\n",
    "    val_results_paths = [p for p in os.listdir(w2s_path) if p.startswith(\"eval_results\") and p[-1].isdigit()]\n",
    "    \n",
    "    w2s_val_results = dict()\n",
    "    for path in val_results_paths:\n",
    "        step = int(path.split(\"_\")[-1])\n",
    "        val_ds: Dataset = load_from_disk(os.path.join(w2s_path, path)).with_format(\"numpy\")  # type: ignore\n",
    "\n",
    "        # NOTE: for a short period, intermediate evals have a different perspective on label names\n",
    "        if 'weak_soft_label' not in val_ds.column_names:\n",
    "            warnings.warn(\"It appears these w2s val results were produced with a version of the code that called weak_soft_label \\\"soft_label\\\"\")\n",
    "            val_ds = val_ds.rename_columns(\n",
    "                {\n",
    "                    \"soft_label\": \"weak_soft_label\",\n",
    "                    \"hard_label\": \"weak_hard_label\",\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        w2s_val_results[step] = SplitResults(\n",
    "            n=len(val_ds),\n",
    "            ids=val_ds[\"id\"],  # type: ignore\n",
    "            weak_soft_labels=val_ds[\"weak_soft_label\"][:, 1],  # type: ignore\n",
    "            gt_soft_labels=val_ds[\"soft_label\"][:, 1] if \"soft_label\" in val_ds.column_names else np.full(len(val_ds), np.inf),  # type: ignore\n",
    "            weak_hard_labels=val_ds[\"weak_hard_label\"],  # type: ignore\n",
    "            gt_hard_labels=val_ds[\"hard_label\"] if \"hard_label\" in val_ds.column_names else np.full(len(val_ds), np.inf),  # type: ignore\n",
    "            logodds=val_ds[\"logit\"][:, 1] - val_ds[\"logit\"][:, 0],  # type: ignore\n",
    "        )\n",
    "    \n",
    "    ### Load test results ###\n",
    "    test_results = dict()\n",
    "    for name, path in zip([\"weak\", \"strong\", \"w2s\"], [weak_path, strong_path, w2s_path]):\n",
    "        test_ds: Dataset = load_from_disk(os.path.join(path, \"eval_results_final\")).with_format(\"numpy\")  # type: ignore\n",
    "        try:\n",
    "            logodds = test_ds[\"logit\"][:, 1] - test_ds[\"logit\"][:, 0]\n",
    "        except KeyError:\n",
    "            warnings.warn(f\"Could not find logit columns in {path}, inferring from \\\"soft_pred\\\" instead\")\n",
    "            p = test_ds[\"soft_pred\"][:, 1]  # type: ignore\n",
    "            logodds = np.log(p / ((1 - p) + 1e-12) + 1e-12)\n",
    "        test_results[name] = SplitResults(\n",
    "            n=len(test_ds),\n",
    "            ids=test_ds[\"id\"],  # type: ignore\n",
    "            weak_soft_labels=test_ds[\"weak_soft_label\"][:, 1] if name == \"w2s\" else np.full(len(test_ds), np.inf),  # type: ignore\n",
    "            gt_soft_labels=test_ds[\"soft_label\"][:, 1],  # type: ignore\n",
    "            weak_hard_labels=(test_ds[\"weak_soft_label\"][:, 1] > 0.5).astype(int) if name == \"w2s\" else np.full(len(test_ds), np.inf),  # type: ignore\n",
    "            gt_hard_labels=test_ds[\"hard_label\"],  # type: ignore\n",
    "            logodds=logodds,\n",
    "        )\n",
    "\n",
    "    ### Load training results ###\n",
    "    # note that in this case \"soft_label\" means weak_soft_label, and there is a \"gt_soft_label\" column\n",
    "    train_ds: Dataset = load_from_disk(os.path.join(w2s_path, \"train_ds\")).with_format(\"numpy\")  # type: ignore\n",
    "    w2s_train_ds = SplitResults(\n",
    "        n=len(train_ds),\n",
    "        ids=train_ds[\"id\"],  # type: ignore\n",
    "        weak_soft_labels=train_ds[\"soft_label\"][:, 1],  # type: ignore\n",
    "        gt_soft_labels=train_ds[\"gt_soft_label\"][:, 1],  # type: ignore\n",
    "        weak_hard_labels=train_ds[\"hard_label\"],  # type: ignore\n",
    "        gt_hard_labels=train_ds[\"gt_hard_label\"],  # type: ignore\n",
    "        logodds=np.full(len(train_ds), np.inf),  # train_ds doesn't contain predictions\n",
    "    )\n",
    "\n",
    "    ### Load gradient results ###\n",
    "    num_steps = max(int(p.split(\"_\")[-1][:-3]) for p in os.listdir(w2s_path) if p.startswith(\"gradients\")) + 1\n",
    "    grad_dict = defaultdict(list)\n",
    "    proj_basis_indices = None\n",
    "    for i in range(num_steps):\n",
    "        grad_batch = torch.load(os.path.join(w2s_path, f\"gradients_{i}.pt\"), map_location=\"cpu\")\n",
    "        this_bs = len(grad_batch[\"ids\"])\n",
    "        grad_dict[\"ids\"].append(np.array(grad_batch[\"ids\"]))\n",
    "        grad_dict[\"steps\"].append(np.array([grad_batch[\"step\"]] * this_bs))\n",
    "        grad_dict[\"lrs\"].append(np.array([grad_batch[\"lr\"]] * this_bs))\n",
    "        grad_dict[\"kernel_grads\"].append(grad_batch[\"expected_effects\"].numpy())\n",
    "        grad_dict[\"test_logodds\"].append(grad_batch[\"eval_outputs\"].expand(this_bs, -1).numpy())\n",
    "        if proj_basis_indices is None:\n",
    "            proj_basis_indices = grad_batch[\"proj_basis_indices\"].numpy()\n",
    "        assert np.all(proj_basis_indices == grad_batch[\"proj_basis_indices\"].numpy())\n",
    "    \n",
    "    grad_dict = {k: np.concatenate(v) for k, v in grad_dict.items()}\n",
    "    \n",
    "    # join the weak_soft_labels and gt_soft_labels from the training set\n",
    "    n_grads = len(grad_dict[\"ids\"])\n",
    "    n_epochs = n_grads // len(w2s_train_ds.ids)\n",
    "    grad_dict[\"weak_soft_labels\"] = w2s_train_ds.weak_soft_labels.repeat(n_epochs + 1)[:n_grads]\n",
    "    grad_dict[\"gt_soft_labels\"] = w2s_train_ds.gt_soft_labels.repeat(n_epochs + 1)[:n_grads]\n",
    "    # ensure the labels are assigned to the corresponding rows\n",
    "    gids = w2s_train_ds.ids.repeat(n_epochs + 1)[:n_grads]\n",
    "    assert np.all(gids == grad_dict[\"ids\"])\n",
    "\n",
    "    grad_results = GradResults(\n",
    "        n_steps=num_steps,\n",
    "        proj_basis_indices=proj_basis_indices,  # type: ignore\n",
    "        **grad_dict\n",
    "    )\n",
    "\n",
    "    ### Compute accuracy metrics ###\n",
    "    accs = {\n",
    "        name: np.mean(test_results[name].gt_hard_labels == (test_results[name].logodds > 0))\n",
    "        for name in [\"weak\", \"strong\", \"w2s\"]\n",
    "    }\n",
    "    pgr = (accs[\"w2s\"] - accs[\"weak\"]) / (accs[\"strong\"] - accs[\"weak\"])\n",
    "\n",
    "    return RunResult(\n",
    "        weak_acc=accs[\"weak\"],\n",
    "        strong_acc=accs[\"strong\"],\n",
    "        w2s_acc=accs[\"w2s\"],\n",
    "        pgr=pgr,\n",
    "        grads=grad_results,\n",
    "        w2s_train=w2s_train_ds,\n",
    "        w2s_vals=w2s_val_results,\n",
    "        w2s_test=test_results[\"w2s\"],\n",
    "        weak_test=test_results[\"weak\"],\n",
    "        strong_test=test_results[\"strong\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_erasure import LeaceEraser\n",
    "from typing import Literal\n",
    "\n",
    "def erase_labels(x, soft_labels, label_erasure: Literal[\"none\", \"leace\", \"mean-diff\", \"keep-negative\", \"keep-positive\"] = \"none\"):\n",
    "    mask = np.full_like(soft_labels, True, dtype=bool)\n",
    "    if label_erasure == \"leace\":\n",
    "        eraser = LeaceEraser.fit(x=torch.from_numpy(x), z=torch.from_numpy(soft_labels > 0.5))\n",
    "        erased = eraser(x=x).numpy()\n",
    "    elif label_erasure == \"mean-diff\":\n",
    "        pos_mean = np.mean(x * soft_labels[:, None], axis=0)\n",
    "        neg_mean = np.mean(x * (1 - soft_labels)[:, None], axis=0)\n",
    "        mean_diff = pos_mean - neg_mean\n",
    "        mean_diff = mean_diff / np.linalg.norm(mean_diff)\n",
    "        erased = x - x @ mean_diff[:, None] * mean_diff[None, :] / (mean_diff @ mean_diff)\n",
    "    elif label_erasure == \"keep-negative\":\n",
    "        mask = soft_labels < 0.5\n",
    "        erased = x[mask]\n",
    "    elif label_erasure == \"keep-positive\":\n",
    "        mask = soft_labels > 0.5\n",
    "        erased = x[mask]\n",
    "    elif label_erasure == \"none\":\n",
    "        erased = x\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label erasure method {label_erasure}\")\n",
    "    return erased, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a72c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quirky sentiment\n",
    "weak_path = \"../results/quirky/sentiment/dn=quirky_sentiment-ntr=3000-nte=300\"\n",
    "strong_path = \"../results/quirky/sentiment/bs=32-dl=1-dn=quir_sent-e=1-ee=50-lp=0-lbmae=1-l=xent-l=1e-05-ls=cosi_anne-mc=512-ms=Mistral-7B-v0.1-sentiment-random-ntd=300-ntd=3000-ntd=0-o=Adam-stl=1-s=0-sg=0-twd=0\"\n",
    "w2s_path = \"../results/quirky/sentiment/bs=128-dl=1-dn=quir_sent-e=1-ee=25-lp=0-lbmae=1-l=kl-l=2e-05-ls=cosi_anne-mc=512-ms=Mistral-7B-v0.1-sentiment-random-ntd=300-ntd=3000-ntd=0-o=Adam-stl=1-s=0-sg=1-twd=0-wms=Bob\"\n",
    "\n",
    "\n",
    "# amazon_polarity\n",
    "# weak_path = \"../results/function-grads/amazon_polarity/bs=32-dl=1-dn=amaz_pola-e=1-ee=50-gib=1-lp=0-lbmae=1-l=xent-l=1e-05-ls=cosi_anne-mc=512-mfbm=auroc-ms=pythia-14m-ntd=500-ntd=10000-ntd=10000-o=Adam-stl=1-s=0-sg=0-twd=0\"\n",
    "# strong_path = \"../results/function-grads/amazon_polarity/bs=32-dl=1-dn=amaz_pola-e=1-ee=50-gib=1-lp=0-lbmae=1-l=xent-l=1e-05-ls=cosi_anne-mc=512-mfbm=auroc-ms=Qwen1.5-0.5B-ntd=500-ntd=10000-ntd=10000-o=Adam-stl=1-s=0-sg=0-twd=0\"\n",
    "# w2s_path = \"../results/function-grads/amazon_polarity/bs=64-dl=1-dn=amaz_pola-e=1-ee=25-gib=1-lp=0-lbmae=1-l=kl-l=1e-05-ls=cosi_anne-mc=512-mfbm=auroc-ms=Qwen1.5-0.5B-ntd=500-ntd=10000-ntd=10000-o=Adam-stl=1-s=0-sg=1-twd=0-wms=pythia-14m\"\n",
    "\n",
    "# mc taco\n",
    "# weak_path = \"../results/function-grads/mc_taco/bs=32-dl=1-dn=mc_taco-e=3-ee=50-gib=1-lp=0-lbmae=1-l=xent-l=1e-05-ls=cosi_anne-mc=512-mfbm=auroc-ms=pythia-410m-ntd=300-ntd=4000-ntd=5000-o=Adam-stl=1-s=0-sg=0-twd=0\"\n",
    "# strong_path = \"../results/function-grads/mc_taco/bs=32-dl=1-dn=mc_taco-e=3-ee=50-gib=1-lp=0-lbmae=1-l=xent-l=1e-05-ls=cosi_anne-mc=512-mfbm=auroc-ms=Qwen1.5-1.8B-ntd=300-ntd=4000-ntd=5000-o=Adam-stl=1-s=0-sg=0-twd=0\"\n",
    "# w2s_path = \"../results/function-grads/mc_taco/bs=64-dl=1-dn=mc_taco-e=2-ee=25-gib=1-lp=0-lbmae=1-l=kl-l=1e-05-ls=cosi_anne-mc=512-mfbm=auroc-ms=Qwen1.5-1.8B-ntd=300-ntd=4000-ntd=5000-o=Adam-stl=1-s=0-sg=1-twd=0-wms=pythia-410m\"\n",
    "\n",
    "# boolq\n",
    "# weak_path = \"../results/function-grads/boolq/bs=32-dl=1-dn=boolq-e=6-ee=50-gib=1-lp=0-lbmae=1-l=xent-l=1e-05-ls=cosi_anne-mc=512-mfbm=auroc-ms=Qwen1.5-0.5B-ntd=500-ntd=4000-ntd=5000-o=Adam-stl=1-s=0-sg=0-twd=0\"\n",
    "# strong_path = \"../results/function-grads/boolq/bs=32-dl=1-dn=boolq-e=6-ee=50-gib=1-lp=0-lbmae=1-l=xent-l=1e-07-ls=cosi_anne-mc=512-mfbm=auroc-ms=Mistral-7B-v0.1-ntd=500-ntd=4000-ntd=5000-o=Adam-stl=1-s=0-sg=0-twd=0\"\n",
    "# w2s_path = \"../results/function-grads/boolq/bs=64-dl=1-dn=boolq-e=2-ee=25-gib=1-lp=0-lbmae=1-l=kl-l=1e-07-ls=cosi_anne-mc=512-mfbm=auroc-ms=Mistral-7B-v0.1-ntd=500-ntd=4000-ntd=5000-o=Adam-stl=1-s=0-sg=1-twd=0-wms=Qwen1.5-0.5B\"\n",
    "\n",
    "# hhrlhf\n",
    "\n",
    "# # sciq\n",
    "# weak_path = \"../results/function-grads/sciq-adam/bs=32-dl=1-dn=sciq-e=5-ee=10000000-gib=1-lp=0-lbmae=0-l=xent-l=1e-05-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=300-ntd=4000-ntd=5500-o=Adam-stl=1-s=0-sg=0-twd=0\"\n",
    "# strong_path = \"../results/function-grads/sciq-adam/bs=32-dl=1-dn=sciq-e=5-ee=10000000-gib=1-lp=0-lbmae=0-l=xent-l=1e-07-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Mistral-7B-v0.1-ntd=300-ntd=4000-ntd=5500-o=Adam-stl=1-s=0-sg=0-twd=0\"\n",
    "# w2s_path = \"../results/function-grads/sciq-adam/bs=64-dl=1-dn=sciq-e=2-ee=10-gib=1-lp=0-lbmae=0-l=kl-l=1e-07-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Mistral-7B-v0.1-ntd=300-ntd=4000-ntd=5500-o=Adam-stl=1-s=0-sg=1-twd=0-wms=Qwen1.5-0.5B\"\n",
    "# SGD\n",
    "# w2s_path = \"../results/function-grads/sciq/bs=64-dl=1-dn=sciq-e=1-ee=25-gib=1-lp=0-lbmae=0-l=kl-l=0.0015-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Mistral-7B-v0.1-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=1-s=0-sg=1-twd=0-wms=Qwen1.5-0.5B\"\n",
    "# strong_path = \"../results/function-grads/sciq/bs=32-dl=1-dn=sciq-e=5-ee=10000000-gib=1-lp=0-lbmae=0-l=xent-l=0.003-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Mistral-7B-v0.1-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=1-s=0-sg=0-twd=0\"\n",
    "# weak_path = \"../results/function-grads/sciq/bs=32-dl=1-dn=sciq-e=5-ee=10000000-gib=1-lp=0-lbmae=0-l=xent-l=0.003-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=1-s=0-sg=0-twd=0\"\n",
    "\n",
    "# # sciq with support\n",
    "# w2s_path = \"../results/function-grads/bs=64-dl=1-dn=sciq_with_supp-e=10-ee=25-gib=1-lp=0-lbmae=0-l=kl-l=8.999999999999999e-05-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=50-s=0-sg=1-twd=0-wms=opt-350m\"\n",
    "# # w2s_path = \"../results/function-grads/bs=64-dl=1-dn=sciq_with_supp-e=2-ee=25-gib=1-lp=0-lbmae=0-l=kl-l=0.00030000000000000003-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=50-s=0-sg=1-twd=0-wms=opt-350m\"\n",
    "# # w2s_path = \"../results/function-grads/bs=64-dl=1-dn=sciq_with_supp-e=2-ee=25-gib=1-lp=0-lbmae=0-l=kl-l=0.003-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=50-s=0-sg=1-twd=0-wms=opt-350m\"\n",
    "# strong_path = \"../results/function-grads/bs=32-dl=1-dn=sciq_with_supp-e=5-ee=10000000-gib=1-lp=0-lbmae=0-l=xent-l=0.003-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=50-s=0-sg=0-twd=0\"\n",
    "# weak_path = \"../results/function-grads/bs=32-dl=1-dn=sciq_with_supp-e=5-ee=10000000-gib=1-lp=0-lbmae=0-l=xent-l=0.003-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=opt-350m-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=50-s=0-sg=0-twd=0\"\n",
    "\n",
    "r = load_run_result(w2s_path, weak_path, strong_path)\n",
    "assert np.all((r.grads.gt_soft_labels == 0) | (r.grads.gt_soft_labels == 1)), \"soft gt labels lead to inaccurate assumptions about weak_error's meaning\"\n",
    "print(f\"Weak acc: {r.weak_acc:.3f}\")\n",
    "print(f\"Strong acc: {r.strong_acc:.3f}\")\n",
    "print(f\"W2S acc: {r.w2s_acc:.3f} (PGR: {r.pgr:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "weak_label_erased, mask = erase_labels(r.grads.kernel_grads, r.grads.weak_soft_labels, label_erasure=\"none\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd1, svd2 = svd.fit_transform(weak_label_erased).T\n",
    "plt.scatter(svd1, svd2, c=r.grads.weak_error[mask], cmap=\"coolwarm\", alpha=0.5, vmin=-1, vmax=1)\n",
    "plt.xlabel(f\"SVD1 (expl. var. ratio: {svd.explained_variance_ratio_[0]:.3f})\")\n",
    "plt.ylabel(f\"SVD2 (expl. var. ratio: {svd.explained_variance_ratio_[1]:.3f})\")\n",
    "\n",
    "print(f\"--- SVD ---\")\n",
    "print(f\"Explained squared-Frobenious norm ratio: {svd.explained_variance_ratio_}\")\n",
    "print(f\"AUROC of y axis {roc_auc_score(np.abs(r.grads.weak_error[mask]) < 0.5, svd2):.3f}\")\n",
    "plt.colorbar(label=\"weak error\")\n",
    "\n",
    "\n",
    "basis1 = np.ones(r.w2s_test.n) / np.sqrt(r.w2s_test.n)\n",
    "\n",
    "for i, yaxis_name in enumerate([\"weak_test_logodds\", \"test_gt_soft_labels\", \"strong_test_logodds\"]):\n",
    "    \n",
    "    if yaxis_name == \"weak_test_logodds\":\n",
    "        yaxis = r.weak_test.logodds\n",
    "    elif yaxis_name == \"test_gt_soft_labels\":\n",
    "        p = r.w2s_test.gt_soft_labels\n",
    "        yaxis = np.log(p / ((1 - p) + 1e-8) + 1e-8)\n",
    "    elif yaxis_name == \"strong_test_logodds\":\n",
    "        yaxis = r.strong_test.logodds\n",
    "\n",
    "    basis2 = yaxis / np.linalg.norm(yaxis)\n",
    "    increase_probs = weak_label_erased @ basis1\n",
    "    report_knowledge = weak_label_erased @ basis2\n",
    "\n",
    "    plt.subplot(2, 2, 2 + i)\n",
    "    plt.scatter(increase_probs, report_knowledge, c=r.grads.weak_error[mask], cmap=\"coolwarm\", alpha=0.5, vmin=-1, vmax=1)\n",
    "\n",
    "    frob_norm = np.sum(weak_label_erased**2)\n",
    "    ev1 = np.sum((weak_label_erased @ basis1)**2) / frob_norm\n",
    "    ev2 = np.sum((weak_label_erased @ basis2)**2) / frob_norm\n",
    "    \n",
    "    plt.xlabel(f\"\\\"increase output probs\\\"$\\\\to$\\n(expl. var. ratio: {ev1:.3f})\")\n",
    "    plt.ylabel(f\"\\\"report your knowledge\\\"$\\\\to$\\n(proj onto {yaxis_name})\\n(expl. var. ratio: {ev2:.3f})\")\n",
    "\n",
    "    print(f\"--- {yaxis_name} ---\")\n",
    "    print(f\"Explained squared-Frobenious norm ratio: {ev1:.3f} (basis1), {ev2:.3f} (basis2)\")\n",
    "    print(f\"AUROC of y axis {roc_auc_score(np.abs(r.grads.weak_error[mask]) < 0.5, report_knowledge):.3f}\")\n",
    "\n",
    "plt.savefig(os.path.join(w2s_path, f\"2D-kernel-grads.pdf\"), bbox_inches=\"tight\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import sigmoid, logsigmoid, kl_div\n",
    "\n",
    "def kl_of_update(start_logodds, delta_logodds):\n",
    "    \"\"\"Elementwise kl divergence induced by delta_logodds update from start_logodds.\n",
    "    Tested against torch.nn.functional.kl_div.\"\"\"\n",
    "    o0, delta = torch.tensor(start_logodds).half(), torch.tensor(delta_logodds).double()\n",
    "    # p0 * (log(p0) - log(p1)) + (1 - p0) * (log(1 - p0) - log(1 - p1))\n",
    "    return (sigmoid(o0) * (logsigmoid(o0) - logsigmoid(o0 + delta)) + \n",
    "        sigmoid(-o0) * (logsigmoid(-o0) - logsigmoid(-o0 - delta))).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_erasure = \"mean-diff\"\n",
    "weak_label_erased, mask = erase_labels(r.grads.kernel_grads, r.grads.weak_soft_labels, label_erasure=label_erasure)\n",
    "\n",
    "### size of [average effect] ###\n",
    "cthresh = 0.3\n",
    "cm = np.abs(r.grads.weak_error) < cthresh\n",
    "im = np.abs(r.grads.weak_error) > 1 - cthresh\n",
    "correct_avg_effect = weak_label_erased[cm].mean(axis=0)\n",
    "incorrect_avg_effect = weak_label_erased[im].mean(axis=0)\n",
    "print(f\"correct if weak_error < {cthresh}: {cm.sum()} examples\")\n",
    "print(f\"incorrect if weak_error > {1 - cthresh}: {im.sum()} examples\")\n",
    "\n",
    "print(f\"L2 norm of [average correct effect]: {np.linalg.norm(correct_avg_effect):.3f}\")\n",
    "print(f\"L2 norm of [average incorrect effect]: {np.linalg.norm(incorrect_avg_effect):.3f}\")\n",
    "\n",
    "assert r.grads.steps[0] == 0, \"test_logodds[0] is not the initial logodds\"\n",
    "kl_correct = kl_of_update(r.grads.test_logodds[0], correct_avg_effect)\n",
    "kl_incorrect = kl_of_update(r.grads.test_logodds[0], incorrect_avg_effect)\n",
    "correct_mean = kl_correct.mean()\n",
    "incorrect_mean = kl_incorrect.mean()\n",
    "correct_stderr = np.std(kl_correct) / np.sqrt(len(kl_correct))\n",
    "incorrect_stderr = np.std(kl_incorrect) / np.sqrt(len(kl_incorrect))\n",
    "print_scale = 10 ** np.ceil(-np.log10(np.abs(correct_mean)))\n",
    "print(f\"KL norm of [average correct effect]: ({print_scale * correct_mean:.3f} +/- {print_scale * 2 * correct_stderr:.3f}) * 10^{-np.log10(print_scale)}\")\n",
    "print(f\"KL norm of [average incorrect effect]: ({print_scale * incorrect_mean:.3f} +/- {print_scale * 2 * incorrect_stderr:.3f}) * 10^{-np.log10(print_scale)}\")\n",
    "\n",
    "### average size of effect ###\n",
    "if label_erasure == \"none\":\n",
    "    print(\"WARNING: norm measurements are not meaningful when label erasure is not applied\")\n",
    "\n",
    "# first using L2\n",
    "correct_norms = np.linalg.norm(weak_label_erased[cm], axis=1)\n",
    "incorrect_norms = np.linalg.norm(weak_label_erased[im], axis=1)\n",
    "correct_avg_norm = np.mean(correct_norms)\n",
    "incorrect_avg_norm = np.mean(incorrect_norms)\n",
    "correct_sem_norm = np.std(correct_norms) / np.sqrt(len(correct_norms))\n",
    "incorrect_sem_norm = np.std(incorrect_norms) / np.sqrt(len(incorrect_norms))\n",
    "print_scale = 10 ** np.ceil(-np.log10(np.abs(correct_avg_norm)))\n",
    "print(\"(mean +/- 2*stderr)\")\n",
    "print(f\"Average [L2 norm of correct effect]: ({print_scale * correct_avg_norm:.2f} +/- {print_scale * 2 * correct_sem_norm:.2f}) * 10^{-np.log10(print_scale)}\")\n",
    "print(f\"Average [L2 norm of incorrect effect]: ({print_scale * incorrect_avg_norm:.2f} +/- {print_scale * 2 * incorrect_sem_norm:.2f}) * 10^{-np.log10(print_scale)}\")\n",
    "\n",
    "# now using KL\n",
    "kl = kl_of_update(r.grads.test_logodds, weak_label_erased).mean(axis=1)\n",
    "\n",
    "correct_kls = kl[cm]\n",
    "incorrect_kls = kl[im]\n",
    "correct_avg_kl = correct_kls.mean()\n",
    "incorrect_avg_kl = incorrect_kls.mean()\n",
    "correct_sem_kl = correct_kls.std() / np.sqrt(len(correct_kls))\n",
    "incorrect_sem_kl = incorrect_kls.std() / np.sqrt(len(incorrect_kls))\n",
    "print_scale = 10 ** np.ceil(-np.log10(np.abs(correct_avg_kl)))\n",
    "print(f\"Average [KL norm of correct effects]: ({print_scale * correct_avg_kl:.2f} +/- {print_scale * 2 * correct_sem_kl:.2f}) * 10^{-np.log10(print_scale)}\")\n",
    "print(f\"Average [KL norm of incorrect effects]: ({print_scale * incorrect_avg_kl:.2f} +/- {print_scale * 2 * incorrect_sem_kl:.2f}) * 10^{-np.log10(print_scale)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a90006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "label_erasure = \"none\"\n",
    "weak_label_erased, mask = erase_labels(r.grads.kernel_grads, r.grads.weak_soft_labels, label_erasure=label_erasure)\n",
    "is_correct = np.abs(r.grads.weak_error[mask]) < 0.5\n",
    "\n",
    "probe_type = \"weak_test_proj\"\n",
    "if probe_type == \"svd2\":\n",
    "    probe = svd.components_[1]\n",
    "elif probe_type == \"gt_test_proj\":\n",
    "    probe = r.w2s_test.gt_hard_labels - np.mean(r.w2s_test.gt_hard_labels)\n",
    "    probe = probe / np.linalg.norm(probe)\n",
    "elif probe_type == \"weak_test_proj\":\n",
    "    probe = r.weak_test.logodds - np.mean(r.weak_test.logodds)\n",
    "    probe = probe / np.linalg.norm(probe)\n",
    "elif probe_type == \"strong_test_proj\":\n",
    "    probe = r.strong_test.logodds - np.mean(r.strong_test.logodds)\n",
    "    probe = probe / np.linalg.norm(probe)\n",
    "elif probe_type == \"lr\" or probe_type == \"mean-diff\":\n",
    "    weak_label_erased = weak_label_erased / np.linalg.norm(weak_label_erased, axis=1)[:, None]\n",
    "    n_train = max(1000, len(weak_label_erased) // 8)\n",
    "    print(f\"Using {n_train} of {len(weak_label_erased)} examples for probe fitting\")\n",
    "    weak_label_erased_train, weak_label_erased = weak_label_erased[:n_train], weak_label_erased[n_train:]\n",
    "    y_train, is_correct = is_correct[:n_train], is_correct[n_train:]\n",
    "    if probe_type == \"lr\":\n",
    "        probe = LogisticRegression(C=1).fit(weak_label_erased_train, y_train).coef_[0]\n",
    "    else:\n",
    "        probe = weak_label_erased_train[y_train].mean(dim=0) - weak_label_erased_train[~y_train].mean(dim=0)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown probe type {probe_type}\")\n",
    "\n",
    "projs = weak_label_erased @ probe\n",
    "percentile = 50\n",
    "proj_thresh = np.percentile(projs, percentile)\n",
    "filter = projs > proj_thresh\n",
    "new_weak_label_acc = np.mean(is_correct[filter])\n",
    "new_pgr = (new_weak_label_acc - r.weak_acc) / (r.strong_acc - r.weak_acc)\n",
    "print(f\"Weak floor val accuracy {r.weak_acc:.4f}\")\n",
    "print(f\"Strong ceiling val accuracy {r.strong_acc:.4f}\")\n",
    "print(f\"Original W2S val accuracy {r.w2s_acc:.4f} (PGR: {r.pgr:.4f})\\n\")\n",
    "print(f\"Probe type: {probe_type}; Label erasure method: {label_erasure}\")\n",
    "print(f\"Weak label accuracy conditional on probe score in >={percentile}th percentile: {new_weak_label_acc:.4f} (PGR*: {new_pgr:.4f})\")\n",
    "print(f\"Probe AUROC at classifying labeling errors: {roc_auc_score(is_correct, projs):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
