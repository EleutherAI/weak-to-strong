{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8553e612",
   "metadata": {},
   "source": [
    "# Calibration w.r.t. the soft labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "\n",
    "w2s_path = \"../results/function-grads/bs=64-dl=1-dn=sciq_with_supp-e=10-ee=25-gib=1-lp=0-lbmae=0-l=kl-l=8.999999999999999e-05-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=50-s=0-sg=1-twd=0-wms=opt-350m\"\n",
    "# w2s_path = \"../results/function-grads/bs=64-dl=1-dn=sciq_with_supp-e=2-ee=25-gib=1-lp=0-lbmae=0-l=kl-l=0.00030000000000000003-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=50-s=0-sg=1-twd=0-wms=opt-350m\"\n",
    "# w2s_path = \"../results/function-grads/bs=64-dl=1-dn=sciq_with_supp-e=2-ee=25-gib=1-lp=0-lbmae=0-l=kl-l=0.003-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=50-s=0-sg=1-twd=0-wms=opt-350m\"\n",
    "strong_path = \"../results/function-grads/bs=32-dl=1-dn=sciq_with_supp-e=5-ee=10000000-gib=1-lp=0-lbmae=0-l=xent-l=0.003-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Qwen1.5-0.5B-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=50-s=0-sg=0-twd=0\"\n",
    "weak_path = \"../results/function-grads/bs=32-dl=1-dn=sciq_with_supp-e=5-ee=10000000-gib=1-lp=0-lbmae=0-l=xent-l=0.003-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=opt-350m-ntd=300-ntd=4000-ntd=5500-o=SGD-stl=50-s=0-sg=0-twd=0\"\n",
    "eval_results_paths = [p for p in os.listdir(w2s_path) if p.startswith(\"eval_results\") and p[-1].isdigit()]\n",
    "eval_results_paths.sort(key=lambda x: int(x.split(\"_\")[-1]))\n",
    "\n",
    "weak_soft_labels = None\n",
    "gt_labels = None\n",
    "pred_probs = []\n",
    "for p in eval_results_paths:\n",
    "    eval_results = load_from_disk(os.path.join(w2s_path, p)).with_format(\"numpy\")\n",
    "    if weak_soft_labels is None:\n",
    "        weak_soft_labels = eval_results['weak_soft_label'][:, 1]  # type: ignore\n",
    "    else:\n",
    "        assert np.all(weak_soft_labels == eval_results['weak_soft_label'][:, 1])  # type: ignore\n",
    "    if gt_labels is None:\n",
    "        gt_labels = eval_results['soft_label'][:, 1]  # type: ignore\n",
    "    else:\n",
    "        assert np.all(gt_labels == eval_results['soft_label'][:, 1])  # type: ignore\n",
    "    pred_probs.append(eval_results['soft_pred'][:, 1])  # type: ignore\n",
    "\n",
    "weak_soft_labels = np.array(weak_soft_labels)\n",
    "pred_probs = np.array(pred_probs)\n",
    "gt_labels = np.array(gt_labels)\n",
    "weak_soft_labels.shape, pred_probs.shape, gt_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf36650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "\n",
    "n_time_steps = len(pred_probs)\n",
    "# Create base figure\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter(x=pred_probs[0], y=weak_soft_labels, mode='markers',  \n",
    "                     marker=dict(color=gt_labels, colorscale='Viridis', colorbar=dict(title='GT Label (t=0)')))],\n",
    "    layout=go.Layout(\n",
    "        title=\"Soft Labels Over Time\",\n",
    "        xaxis=dict(range=[0, 1], title=\"Strong student predicted probability\"),\n",
    "        yaxis=dict(range=[0, 1], title=\"Weak supervisor soft label\"),\n",
    "        updatemenus=[dict(\n",
    "            type=\"buttons\",\n",
    "            buttons=[dict(label=\"Play\",\n",
    "                          method=\"animate\",\n",
    "                          args=[None, {\"frame\": {\"duration\": 700, \"redraw\": True}, \"fromcurrent\": True}]),\n",
    "                    dict(label=\"Pause\",\n",
    "                         method=\"animate\",\n",
    "                         args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False}, \"mode\": \"immediate\"}])\n",
    "                    ])]\n",
    "    ),\n",
    "    frames=[go.Frame(data=[go.Scatter(x=pred_probs[i], y=weak_soft_labels, mode='markers',\n",
    "                                        marker=dict(color=gt_labels, colorscale='Viridis', colorbar=dict(title=f'GT Label (t={i})')))],\n",
    "                         name=str(i))\n",
    "            for i in range(1, n_time_steps)]\n",
    ")\n",
    "\n",
    "# Add axis titles\n",
    "fig.update_layout(xaxis_title=\"Strong student predicted probability\", yaxis_title=\"Weak supervisor soft label\", height=800, width=900)\n",
    "\n",
    "\n",
    "# Show animation\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1665f",
   "metadata": {},
   "source": [
    "# Validate linear approximation of effect of train examples on model behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde128ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "num_steps = max(int(p.split(\"_\")[-1][:-3]) for p in os.listdir(w2s_path) if p.startswith(\"gradients\")) + 1\n",
    "\n",
    "key = \"eval_outputs\"\n",
    "# key = \"eval_probs\"\n",
    "initial_eval_outputs = None\n",
    "approx_eval_outputs = None\n",
    "last_eval_outputs = None\n",
    "eval_outputs = []\n",
    "examplewise_expected_effects = []\n",
    "grads_ids = []\n",
    "\n",
    "# TODO: check that ids are the same\n",
    "for step in range(num_steps):\n",
    "    grads_dict = torch.load(f\"{w2s_path}/gradients_{step}.pt\", map_location=\"cpu\")\n",
    "    if initial_eval_outputs is None or approx_eval_outputs is None:\n",
    "        initial_eval_outputs = grads_dict[key]\n",
    "        approx_eval_outputs = grads_dict[key]\n",
    "\n",
    "    last_eval_outputs = grads_dict[key]\n",
    "    eval_outputs.append(last_eval_outputs.repeat((len(grads_dict[\"ids\"]), 1)))\n",
    "    # NOTE: due to a bug some of the expected effects had unmodified rows from torch.empty, so\n",
    "    # we remove them here\n",
    "    expected_effects = grads_dict[\"expected_effects\"][:len(grads_dict[\"ids\"])]\n",
    "    approx_eval_outputs = approx_eval_outputs.add(expected_effects.sum(dim=0))\n",
    "    examplewise_expected_effects.append(expected_effects)\n",
    "    grads_ids.extend(grads_dict[\"ids\"])\n",
    "\n",
    "    if len(grads_dict[\"ids\"]) != len(grads_dict[\"expected_effects\"]):\n",
    "        print(f\"Step {step} has different number of ids and expected effects {len(grads_dict['ids'])} vs {len(grads_dict['expected_effects'])}\")\n",
    "\n",
    "eval_outputs = torch.cat(eval_outputs, dim=0)  # [n_train, n_eval]\n",
    "examplewise_expected_effects = torch.cat(examplewise_expected_effects, dim=0)  # [n_train, n_eval]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(last_eval_outputs, approx_eval_outputs)\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "plt.axvline(0, color=\"black\", linestyle=\"--\")\n",
    "plt.xlabel(\"True eval outputs\")\n",
    "plt.ylabel(\"Approx eval outputs\")\n",
    "plt.title(f\"When linearly approximating effect on model's eval *logodds* over {num_steps} SGD steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ce761",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_from_disk(os.path.join(w2s_path, \"train_ds\")).with_format(\"numpy\")\n",
    "n_train, n_eval = examplewise_expected_effects.shape\n",
    "gt_hard_labels = train_ds[\"gt_hard_label\"][:n_train]\n",
    "weak_soft_labels = train_ds[\"soft_label\"][:n_train, 1]\n",
    "ids = train_ds[\"id\"][:n_train]\n",
    "if len(gt_hard_labels) < n_train:\n",
    "    gt_hard_labels = np.concatenate([gt_hard_labels] * (n_train // len(gt_hard_labels) + 1))[:n_train]\n",
    "    weak_soft_labels = np.concatenate([weak_soft_labels] * (n_train // len(weak_soft_labels) + 1))[:n_train]\n",
    "    ids = np.concatenate([ids] * (n_train // len(ids) + 1))[:n_train]\n",
    "weak_error = weak_soft_labels - gt_hard_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2s_eval_ds = load_from_disk(os.path.join(w2s_path, f\"eval_results_{num_steps - num_steps % 25}\")).with_format(\"numpy\")\n",
    "strong_eval_ds = load_from_disk(os.path.join(strong_path, \"eval_results_final\")).with_format(\"numpy\")\n",
    "weak_eval_ds = load_from_disk(os.path.join(weak_path, \"eval_results_final\")).with_format(\"numpy\")\n",
    "eval_gt_hard_labels = w2s_eval_ds[\"hard_label\"]\n",
    "strong_eval_logodds = strong_eval_ds[\"logit\"][:, 1] - strong_eval_ds[\"logit\"][:, 0]\n",
    "weak_eval_logodds = weak_eval_ds[\"logit\"][:, 1] - weak_eval_ds[\"logit\"][:, 0]\n",
    "w2s_eval_logodds = w2s_eval_ds[\"logit\"][:, 1] - w2s_eval_ds[\"logit\"][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load train dataset to get `gt_hard_label` and `soft_label` \n",
    "# and assert ids match\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "color = \"weak_error\"\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "examplewise_expected_effects_reduced = svd.fit_transform(examplewise_expected_effects)\n",
    "plt.scatter(examplewise_expected_effects_reduced[:, 0], examplewise_expected_effects_reduced[:, 1], c=locals()[color], cmap=\"coolwarm\", alpha=0.5,)\n",
    "# plt.colorbar(label=color)\n",
    "plt.xlabel(\"SVD1\")\n",
    "plt.ylabel(\"SVD2\")\n",
    "\n",
    "print(f\"Explained variance ratio: {svd.explained_variance_ratio_}\")\n",
    "print(f\"AUROC of y axis {roc_auc_score(np.abs(weak_error) < 0.5, examplewise_expected_effects_reduced[:, 1])}\")\n",
    "\n",
    "from copy import deepcopy\n",
    "custom_proj = deepcopy(svd)\n",
    "custom_proj.components_[0] = np.ones(n_eval) / np.sqrt(n_eval)\n",
    "yaxis_name = \"eval_gt_hard_labels\"\n",
    "# yaxis_name = \"weak_eval_logodds\"\n",
    "yaxis = locals()[yaxis_name] - np.mean(locals()[yaxis_name])\n",
    "custom_proj.components_[1] = yaxis / np.linalg.norm(yaxis)\n",
    "examplewise_expected_effects_reduced = custom_proj.transform(examplewise_expected_effects)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(examplewise_expected_effects_reduced[:, 0], examplewise_expected_effects_reduced[:, 1], c=locals()[color], cmap=\"coolwarm\", alpha=0.5,)\n",
    "# plt.xlim(-0.2, 0.2)\n",
    "# plt.ylim(-0.02, 0.02)\n",
    "plt.colorbar(label=color)\n",
    "plt.xlabel(\"\\\"increase output probs\\\"$\\\\to$\")\n",
    "plt.ylabel(f\"\\\"report your knowledge\\\"$\\\\to$\\n(proj onto {yaxis_name})\")\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"Explained variance ratio: {custom_proj.explained_variance_ratio_}\")\n",
    "print(f\"AUROC of y axis {roc_auc_score(np.abs(weak_error) < 0.5, examplewise_expected_effects_reduced[:, 1])}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept_erasure import LeaceEraser\n",
    "from torch.nn.functional import sigmoid, logsigmoid\n",
    "\n",
    "label_erasure = \"mean-diff\"\n",
    "if label_erasure == \"leace\":\n",
    "    eraser = LeaceEraser.fit(x=examplewise_expected_effects, z=torch.from_numpy(weak_soft_labels > 0.5))\n",
    "    weak_label_erased = eraser(x=examplewise_expected_effects)\n",
    "elif label_erasure == \"mean-diff\":\n",
    "    pos_mean = (examplewise_expected_effects * weak_soft_labels[:, None]).mean(dim=0)\n",
    "    neg_mean = (examplewise_expected_effects * (1 - weak_soft_labels)[:, None]).mean(dim=0)\n",
    "    mean_diff = pos_mean - neg_mean\n",
    "    mean_diff = mean_diff / mean_diff.norm()\n",
    "    weak_label_erased = examplewise_expected_effects - examplewise_expected_effects @ mean_diff[:, None] * mean_diff[None, :] / (mean_diff @ mean_diff)\n",
    "elif label_erasure == \"none\":\n",
    "    weak_label_erased = examplewise_expected_effects\n",
    "else:\n",
    "    raise ValueError(f\"Unknown label erasure method {label_erasure}\")\n",
    "\n",
    "# elementwise kl divergence\n",
    "# p0 * (log(p0) - log(p1)) + (1 - p0) * (log(1 - p0) - log(1 - p1))\n",
    "kl = sigmoid(eval_outputs) * (logsigmoid(eval_outputs) - logsigmoid(eval_outputs + weak_label_erased)) + \\\n",
    "        sigmoid(-eval_outputs) * (logsigmoid(-eval_outputs) - logsigmoid(-eval_outputs - weak_label_erased))\n",
    "kl = kl.mean(dim=1)\n",
    "\n",
    "correct_mask = np.abs(weak_error) < 0.5\n",
    "print(f\"Correct predictions: {np.mean(correct_mask)}\")\n",
    "correct_avg_effect = weak_label_erased[correct_mask].mean(dim=0)\n",
    "incorrect_avg_effect = weak_label_erased[~correct_mask].mean(dim=0)\n",
    "correct_std_effect = weak_label_erased[correct_mask].std(dim=0)\n",
    "incorrect_std_effect = weak_label_erased[~correct_mask].std(dim=0)\n",
    "\n",
    "if label_erasure == \"none\":\n",
    "    print(\"WARNING: norm measurements are not meaningful when label erasure is not applied\")\n",
    "correct_norms = kl[np.abs(weak_error) < 0.1]\n",
    "incorrect_norms = kl[np.abs(weak_error) > 0.9]\n",
    "correct_avg_norm = correct_norms.mean()\n",
    "incorrect_avg_norm = incorrect_norms.mean()\n",
    "correct_sem_norm = correct_norms.std() / np.sqrt(len(correct_norms))\n",
    "incorrect_sem_norm = incorrect_norms.std() / np.sqrt(len(incorrect_norms))\n",
    "print_scale = 1e8\n",
    "print(f\"Correct avg norm: ({print_scale * correct_avg_norm:.2f} +/- {print_scale * 2 * correct_sem_norm:.2f}) * 10^{-np.log10(print_scale)}\")\n",
    "print(f\"Incorrect avg norm: ({print_scale * incorrect_avg_norm:.2f} +/- {print_scale * 2 * incorrect_sem_norm:.2f}) * 10^{-np.log10(print_scale)}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "proj = custom_proj\n",
    "incorrect_2d = proj.transform(incorrect_avg_effect[None, :])\n",
    "correct_2d = proj.transform(correct_avg_effect[None, :])\n",
    "incorrect_2d_std = proj.transform(incorrect_std_effect[None, :])\n",
    "correct_2d_std = proj.transform(correct_std_effect[None, :])\n",
    "\n",
    "plt.plot([0, incorrect_2d[0, 0]], [0, incorrect_2d[0, 1]], label=\"Incorrect\", color=\"red\")\n",
    "plt.plot([0, correct_2d[0, 0]], [0, correct_2d[0, 1]], label=\"Correct\", color=\"green\")\n",
    "plt.scatter(incorrect_2d[0, 0], incorrect_2d[0, 1], color=\"red\")\n",
    "plt.scatter(correct_2d[0, 0], correct_2d[0, 1], color=\"green\")\n",
    "zoom = 10\n",
    "plt.xlim(-0.15 / zoom, 0.15 / zoom)\n",
    "plt.ylim(-0.03 / zoom, 0.03 / zoom)\n",
    "\n",
    "if proj is svd:\n",
    "    plt.xlabel(\"SVD1\")\n",
    "    plt.ylabel(\"SVD2\")\n",
    "elif proj is custom_proj:\n",
    "    plt.xlabel(\"\\\"increase output probs\\\"$\\\\to$\")\n",
    "    plt.ylabel(f\"\\\"report your knowledge\\\"$\\\\to$\\n(proj onto {yaxis_name})\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown projection {proj}\")\n",
    "\n",
    "plt.title(f\"Average effect of train examples (zoomed {zoom}x)\")\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "plt.axvline(0, color=\"black\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a90006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "is_correct = np.abs(weak_error) < 0.5\n",
    "\n",
    "probe_type = \"mean-diff\"\n",
    "if probe_type == \"svd2\":\n",
    "    probe = svd.components_[1]\n",
    "elif probe_type == \"gt_eval_proj\":\n",
    "    probe = eval_gt_hard_labels - np.mean(eval_gt_hard_labels)\n",
    "    probe = probe / np.linalg.norm(probe)\n",
    "elif probe_type == \"weak_eval_proj\":\n",
    "    probe = weak_eval_logodds - np.mean(weak_eval_logodds)\n",
    "    probe = probe / np.linalg.norm(probe)\n",
    "elif probe_type == \"strong_eval_proj\":\n",
    "    probe = strong_eval_logodds - np.mean(strong_eval_logodds)\n",
    "    probe = probe / np.linalg.norm(probe)\n",
    "elif probe_type == \"logistic_regression\":\n",
    "    x = examplewise_expected_effects / examplewise_expected_effects.norm(dim=1)[:, None]\n",
    "    probe = LogisticRegression().fit(x, is_correct).coef_[0]\n",
    "elif probe_type == \"mean-diff\":\n",
    "    probe = examplewise_expected_effects[is_correct].mean(dim=0) - examplewise_expected_effects[~is_correct].mean(dim=0)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown probe type {probe_type}\")\n",
    "\n",
    "projs = examplewise_expected_effects @ probe\n",
    "percentile = 90\n",
    "proj_thresh = np.percentile(projs, percentile)\n",
    "mask = projs > proj_thresh\n",
    "floor = np.mean((weak_eval_logodds > 0) == eval_gt_hard_labels)\n",
    "ceil = np.mean((strong_eval_logodds > 0) == eval_gt_hard_labels)\n",
    "w2s_eval_acc = np.mean((w2s_eval_logodds > 0) == eval_gt_hard_labels)\n",
    "original_pgr = (w2s_eval_acc - floor) / (ceil - floor)\n",
    "new_weak_label_acc = np.mean(is_correct[mask])\n",
    "new_pgr = (new_weak_label_acc - floor) / (ceil - floor)\n",
    "print(f\"Probe type: {probe_type}\")\n",
    "print(f\"Weak floor val accuracy {floor:.4f}\")\n",
    "print(f\"Strong ceiling val accuracy {ceil:.4f}\")\n",
    "print(f\"Original W2S val accuracy {w2s_eval_acc:.4f} (PGR: {original_pgr:.4f})\")\n",
    "print(f\"Weak label accuracy conditional on probe score in >={percentile}th percentile: {new_weak_label_acc:.4f} (PGR: {new_pgr:.4f})\")\n",
    "print(f\"Probe AUROC at classifying labeling errors: {roc_auc_score(is_correct, projs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c64f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "# print(np.mean(svd.components_[1] * (2 * eval_gt_hard_labels - 1) >= 0))\n",
    "# print(np.mean(svd.components_[1] * strong_eval_logodds >= 0))\n",
    "# print(np.mean(svd.components_[1] * weak_eval_logodds >= 0))\n",
    "print(cossim(svd.components_[1], (2 * eval_gt_hard_labels - 1)))\n",
    "print(cossim(svd.components_[1], strong_eval_logodds))\n",
    "print(cossim(svd.components_[1], weak_eval_logodds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0caeef6",
   "metadata": {},
   "source": [
    "# Get gradients per example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "proj_grads = torch.load(os.path.join(w2s_path, \"proj_grads.pt\"), map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "\n",
    "strong_eval_results_path = os.path.join(strong_path, \"eval_results_final\")\n",
    "final_eval_results_path = os.path.join(w2s_path, \"eval_results_final\")\n",
    "final_eval_results = load_from_disk(final_eval_results_path)\n",
    "strong_eval_results = load_from_disk(strong_eval_results_path)\n",
    "print(len(final_eval_results), len(strong_eval_results))\n",
    "strong_eval_results = strong_eval_results.select(range(len(final_eval_results)))\n",
    "assert np.all(np.array(final_eval_results['id']) == np.array(strong_eval_results['id']))\n",
    "final_eval_results = final_eval_results.add_column(\"strong_soft_pred\", strong_eval_results['soft_pred'])  # type: ignore\n",
    "final_eval_results = final_eval_results.add_column(\"strong_hard_pred\", strong_eval_results['hard_pred'])  # type: ignore\n",
    "final_eval_results = final_eval_results.with_format(\"torch\")\n",
    "\n",
    "print(\"weak_hard_label acc\", ((final_eval_results['weak_soft_label'][:, 1] > 0.5) == final_eval_results['hard_label']).float().mean())\n",
    "print(\"strong_hard_pred acc\", (final_eval_results['strong_hard_pred'] == final_eval_results['hard_label']).float().mean())\n",
    "print(\"w2s pred acc\", (final_eval_results['hard_pred'] == final_eval_results['hard_label']).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ccc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_str(txt):\n",
    "    last_quote = txt.rfind('\"')\n",
    "    penultimate_quote = txt.rfind('\"', 0, last_quote)\n",
    "    return txt[penultimate_quote + 1:last_quote]\n",
    "is_answer_in_support = np.array([s.count(get_answer_str(s)) > 1 for s in final_eval_results[\"txt\"]]) \n",
    "exists_support = np.array([not s.startswith(\"Name: Bob\\n\\nPassage 1:\\n\\n\\nQ1:\") for s in final_eval_results[\"txt\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f2ec4",
   "metadata": {},
   "source": [
    "# animated plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1841f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from umap import UMAP\n",
    "from typing import Literal\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "soft_labels = np.array(final_eval_results[\"soft_label\"])[:, 1]\n",
    "weak_soft_labels = np.array(final_eval_results[\"weak_soft_label\"])[:, 1]\n",
    "strong_soft_preds = np.array(final_eval_results[\"strong_soft_pred\"])[:, 1]\n",
    "w2s_soft_preds = np.array(final_eval_results[\"soft_pred\"])[:, 1]\n",
    "\n",
    "weak_error = (weak_soft_labels - soft_labels)\n",
    "weak_diff_from_strong = (weak_soft_labels - strong_soft_preds)\n",
    "\n",
    "run_type = \"w2s\"\n",
    "target_label_column = \"weak_soft_label\"\n",
    "checkpoints = list(s for s in proj_grads[run_type].keys())\n",
    "frames_data = []\n",
    "colors = []\n",
    "\n",
    "dim_reduction: Literal[\"PCA\", \"TruncatedSVD\", \"UMAP\"] = \"PCA\"\n",
    "proj_class, proj_kwargs = {\n",
    "    \"PCA\": (PCA, {}),\n",
    "    \"TruncatedSVD\": (TruncatedSVD, {}),\n",
    "    \"UMAP\": (UMAP, {\"n_neighbors\": 15, \"metric\": \"euclidean\", \"random_state\": 0, \"densmap\": True})\n",
    "}[dim_reduction]\n",
    "projector = None\n",
    "for ckpt_key in checkpoints[:1]:\n",
    "    ckpt_grads = proj_grads[\"w2s\"][ckpt_key][target_label_column]\n",
    "    \n",
    "    # outlier detection: remove outliers\n",
    "    out_detector = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "    inliers = out_detector.fit_predict(ckpt_grads.cpu().numpy()) == 1\n",
    "    percent_outliers = 1 - inliers.mean()\n",
    "    print(f\"Outliers: {percent_outliers:.1%}\")\n",
    "    \n",
    "    ckpt_grads = ckpt_grads[inliers]\n",
    "    color = weak_error[inliers]\n",
    "\n",
    "    # # project away from the difference in means directions\n",
    "    # l = final_eval_results[\"weak_soft_label\"][inliers][:, 1] > 0.5\n",
    "    # mean_diff = ckpt_grads[l == 1].mean(dim=0) - ckpt_grads[l == 0].mean(dim=0)\n",
    "    # ckpt_grads = ckpt_grads - ckpt_grads @ mean_diff[:, None] * mean_diff[None, :] / (mean_diff @ mean_diff)\n",
    "\n",
    "    if projector is None:\n",
    "        projector = proj_class(n_components=2, **proj_kwargs)\n",
    "        projector.fit(ckpt_grads.cpu().numpy())\n",
    "    proj_2d = projector.transform(ckpt_grads.cpu().numpy())\n",
    "\n",
    "    colors.append(color)\n",
    "    frames_data.append(proj_2d)\n",
    "\n",
    "# Prepare the animated plot\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "traces = [go.Scatter(\n",
    "    x=frames_data[0][:, 0], y=frames_data[0][:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=6, color=colors[0],  # assuming weak_error is properly calculated per checkpoint\n",
    "                colorscale='RdBu', showscale=True),\n",
    "    text=np.arange(len(frames_data[0])),\n",
    "    hoverinfo='text'\n",
    ")]\n",
    "\n",
    "fig.add_traces(traces)\n",
    "\n",
    "# Add frames\n",
    "frames = [go.Frame(\n",
    "    data=[go.Scatter(\n",
    "        x=frame[:, 0], y=frame[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color=color,  # assuming weak_error recalculated for each checkpoint\n",
    "                    colorscale='RdBu', showscale=True),\n",
    "        text=np.arange(len(frame))\n",
    "    )],\n",
    "    name=str(ckpt)\n",
    ") for frame, ckpt, color in zip(frames_data, checkpoints, colors)]\n",
    "\n",
    "fig.frames = frames\n",
    "\n",
    "# Add slider\n",
    "sliders = [dict(\n",
    "    steps=[dict(method='animate',\n",
    "                args=[[str(ckpt)],\n",
    "                      dict(mode='immediate',\n",
    "                           frame=dict(duration=500, redraw=True),\n",
    "                           fromcurrent=True)],\n",
    "                label=str(ckpt)) for ckpt in checkpoints],\n",
    "    transition={'duration': 400},\n",
    "    x=0,\n",
    "    y=0,\n",
    "    currentvalue=dict(font=dict(size=12), visible=True),\n",
    "    len=1.0\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text=f'{dim_reduction} Projection of Gradients Over Time'),\n",
    "    sliders=sliders,\n",
    "    # xaxis=dict(title='PC1', range=[-1.5, 1.5]),\n",
    "    # yaxis=dict(title='PC2', range=[-0.5, .5]),\n",
    "    height=600, width=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd00650",
   "metadata": {},
   "source": [
    "# Homogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from concept_erasure import LeaceEraser\n",
    "\n",
    "def get_ckpt_grads_and_weak_error(\n",
    "    proj_grads = proj_grads,\n",
    "    ckpt: str = \"checkpoint_0.bin\",\n",
    "    target_label_column = \"weak_soft_label\",\n",
    "    run_type = \"w2s\",\n",
    "    label_erasure: Literal[\"keep-negative\", \"keep-positive\", \"leace\", \"mean-diff\", \"none\"] = \"none\",\n",
    "):\n",
    "\n",
    "    ckpt_grads = proj_grads[run_type][ckpt][target_label_column].cpu()\n",
    "    n_eval = ckpt_grads.size(0)\n",
    "\n",
    "    soft_labels = np.array(final_eval_results[\"soft_label\"])[:n_eval, 1]\n",
    "    weak_soft_labels = np.array(final_eval_results[\"weak_soft_label\"])[:n_eval, 1]\n",
    "    strong_soft_preds = np.array(final_eval_results[\"strong_soft_pred\"])[:n_eval, 1]\n",
    "    w2s_soft_preds = np.array(final_eval_results[\"soft_pred\"])[:n_eval, 1]\n",
    "\n",
    "    weak_error = (weak_soft_labels - soft_labels)\n",
    "    weak_diff_from_strong = (weak_soft_labels - strong_soft_preds)\n",
    "\n",
    "\n",
    "    if label_erasure == \"keep-negative\" or label_erasure == \"keep-positive\":\n",
    "        # condition on weak_label > 0.5 or weak_label < 0.5\n",
    "        mask = weak_soft_labels > 0.5 if label_erasure == \"keep-positive\" else weak_soft_labels < 0.5\n",
    "        ckpt_grads = ckpt_grads[mask]\n",
    "        weak_error = weak_error[mask]\n",
    "        weak_diff_from_strong = weak_diff_from_strong[mask]\n",
    "    elif label_erasure == \"leace\":\n",
    "        eraser = LeaceEraser.fit(x=ckpt_grads, z=torch.from_numpy(weak_soft_labels > 0.5))\n",
    "        ckpt_grads = eraser(x=ckpt_grads)\n",
    "    elif label_erasure == \"mean-diff\":\n",
    "        pos_mean = (ckpt_grads * weak_soft_labels[:, None]).mean(dim=0)\n",
    "        neg_mean = (ckpt_grads * (1 - weak_soft_labels)[:, None]).mean(dim=0)\n",
    "        mean_diff = pos_mean - neg_mean\n",
    "        mean_diff = mean_diff / mean_diff.norm()\n",
    "        ckpt_grads = ckpt_grads - ckpt_grads @ mean_diff[:, None] * mean_diff[None, :] / (mean_diff @ mean_diff)\n",
    "    elif label_erasure != \"none\":\n",
    "        raise ValueError(f\"Unknown label erasure method: {label_erasure}\")\n",
    "\n",
    "    return ckpt_grads, weak_error, weak_diff_from_strong\n",
    "\n",
    "def cfg_str(cfg):\n",
    "    # break lines every 2 items\n",
    "    s = cfg.__repr__()\n",
    "    # replace every second \"', '\" with \"',\\n'\"\n",
    "    comma_idxs = [i for i, c in enumerate(s) if s[i:i+4] == \"', '\"][1::2]\n",
    "    for i in reversed(comma_idxs):\n",
    "        s = s[:i] + \"',\\n'\" + s[i+4:]\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f37e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = dict(\n",
    "    ckpt = \"checkpoint_0.bin\",\n",
    "    run_type = \"w2s\",\n",
    "    target_label_column = \"weak_soft_label\",\n",
    "    label_erasure=\"leace\"\n",
    ")\n",
    "\n",
    "ckpt_grads, weak_error, weak_diff_from_strong = get_ckpt_grads_and_weak_error(proj_grads, **cfg)\n",
    "use_euclidean = False\n",
    "\n",
    "ckpt_grads, weak_error, weak_diff_from_strong = get_ckpt_grads_and_weak_error(proj_grads, ckpt, target_label_column, run_type, label_erasure=\"keep-negative\")\n",
    "\n",
    "weak_error_argsorted = np.abs(weak_error).argsort()\n",
    "sorted_ckpt_grads = ckpt_grads[weak_error_argsorted]\n",
    "sorted_weak_error = np.abs(weak_error[weak_error_argsorted])\n",
    "pca = PCA(n_components=30)\n",
    "proj = pca.fit_transform(sorted_ckpt_grads.cpu().numpy())\n",
    "proj_unit = proj / np.linalg.norm(proj, axis=1)[:, None]\n",
    "\n",
    "# take pairwise euclidean difference between all pairs of vectors\n",
    "if use_euclidean:\n",
    "    dist = np.add.outer((proj ** 2).sum(axis=1), (proj ** 2).sum(axis=1)) - 2 * proj @ proj.T\n",
    "else:\n",
    "    dist = 1 - proj_unit @ proj_unit.T\n",
    "np.abs(dist).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbabfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(dist, cmap='coolwarm', origin='lower', interpolation='nearest')\n",
    "plt.colorbar(label=\"Euclidean distance (using rank 100 PCA)\" if use_euclidean else \"1 - cosine similarity\")\n",
    "plt.title(f\"distance between sample gradients\\nconditioned on {cfg['label_erasure']} weak label\")\n",
    "plt.xlabel(\"abs(weak_error) rank ->\")\n",
    "plt.ylabel(\"abs(weak_error) rank ->\")\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "k = 100\n",
    "avg_dist = np.zeros_like(sorted_weak_error)\n",
    "for i in range(0, len(sorted_weak_error)):\n",
    "    avg_dist[i] = np.mean(dist[i, max(i - k // 2, 0):i + k // 2 + 1]) \n",
    "r, p = pearsonr(range(len(weak_error)), avg_dist)\n",
    "print(f\"Pearson correlation: {r:.4f}, p-value: {p:.4f}\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(range(len(weak_error)), avg_dist, s=1, alpha=0.5)\n",
    "plt.xlabel(\"abs(weak_error) rank ->\")\n",
    "plt.ylabel(f\"avg distance from {k} samples with most similar abs(weak_error)\")\n",
    "plt.title(\"weak_error rank vs homogeneity\")\n",
    "plt.text(0.5, 0.1, f\"Pearson correlation: {r:.4f}\\np-value: {p:.4f}\", transform=plt.gca().transAxes, ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you've LEACE'd or conditioned away the weak label, there's two different kinds of gradients for a binary classification task: On a spectrum from\n",
    "# 1. Gradients that cause the model to predict what it believes\n",
    "# 2. Gradients that cause the model to predict the opposite of what it believes\n",
    "# These are indistinguishable until you have a dataset that agrees with the model's beliefs more often than not\n",
    "# Then you should be able to see a main cluster of points reinforcing the model's beliefs (1), a smaller\n",
    "# cluster of points that do the opposite (2), and outliers/background points (3) that neither reinforce nor oppose the model's beliefs.\n",
    "# I think (2) and (3) correspond to mislabeled examples that should be thrown out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa99ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glosh  https://hdbscan.readthedocs.io/en/latest/outlier_detection.html\n",
    "import hdbscan\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=None, cluster_selection_epsilon=0, metric=\"euclidean\").fit(proj)\n",
    "pearsonr(clusterer.outlier_scores_, sorted_weak_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(sorted_weak_error > 0.5, clusterer.outlier_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd180680",
   "metadata": {},
   "source": [
    "# Try to probe for false positives and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b53db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = dict(\n",
    "    ckpt = \"checkpoint_0.bin\",\n",
    "    run_type = \"w2s\",\n",
    "    target_label_column = \"weak_soft_label\",\n",
    "    label_erasure=\"none\"\n",
    ")\n",
    "\n",
    "ckpt_grads, weak_error, weak_diff_from_strong = get_ckpt_grads_and_weak_error(proj_grads, **cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be423b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_weak_correct = np.abs(weak_error) < 0.5\n",
    "arange = np.arange(len(ckpt_grads))\n",
    "np.random.shuffle(arange)\n",
    "train_idx, val_idx = np.split(arange, [int(0.8 * len(ckpt_grads))])\n",
    "train_grads, val_grads = ckpt_grads[train_idx], ckpt_grads[val_idx]\n",
    "train_weak_correct, val_weak_correct = torch.from_numpy(is_weak_correct[train_idx]), torch.from_numpy(is_weak_correct[val_idx])\n",
    "\n",
    "pca = PCA(n_components=min(100, train_grads.size(0)))\n",
    "train_proj = torch.from_numpy(pca.fit_transform(train_grads.cpu().numpy()))\n",
    "val_proj = torch.from_numpy(pca.transform(val_grads.cpu().numpy()))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "lam = 0.01\n",
    "probe = LogisticRegression(C=1/lam, penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "# probe = MeanDiffReporter(100, torch.device('cpu'), torch.float32)\n",
    "probe.fit(train_proj, train_weak_correct)\n",
    "# probe.coef_ = np.zeros_like(probe.coef_)\n",
    "# probe.coef_[0] = 1\n",
    "train_scores = probe.decision_function(train_proj)\n",
    "val_scores = probe.decision_function(val_proj)\n",
    "# train_loss = np.mean((train_scores - train_weak_error) ** 2)\n",
    "# val_loss = np.mean((val_scores - val_weak_error) ** 2)\n",
    "# print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
    "train_auroc = roc_auc_score(train_weak_correct.detach().numpy(), train_scores)\n",
    "val_auroc = roc_auc_score(val_weak_correct.detach().numpy(), val_scores)\n",
    "print(f\"Train AUROC: {train_auroc:.4f}, Val AUROC: {val_auroc:.4f}\")\n",
    "\n",
    "proj_onto_coef = val_proj @ probe.coef_.flatten()\n",
    "proj_away_coef = ((val_proj - proj_onto_coef[:, None]) ** 2).sum(axis=1) ** 0.5\n",
    "proj_pc0 = val_proj[:, 0]\n",
    "plt.scatter(proj_onto_coef, val_proj[:, 0], c=val_weak_correct, cmap='coolwarm', alpha=0.5, vmin=0, vmax=1)\n",
    "plt.xlabel(\"Projection onto probe\")\n",
    "plt.ylabel(\"Projection onto PC1\")\n",
    "plt.colorbar(label=\"Is weak label correct?\")\n",
    "plt.title(f\"AUROC: {val_auroc:.4f}\\n{cfg_str(cfg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ea4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(probe.coef_.flatten())), probe.coef_.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f7f9f1",
   "metadata": {},
   "source": [
    "# comparing gradients with gt vs weak supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_cfg = dict(\n",
    "    ckpt = \"checkpoint_150.bin\",\n",
    "    run_type = \"w2s\",\n",
    "    target_label_column = \"weak_soft_label\",\n",
    "    label_erasure=\"mean-diff\"\n",
    ")\n",
    "gt_config = weak_cfg.copy()\n",
    "gt_config['target_label_column'] = \"soft_label\"\n",
    "\n",
    "weak_ckpt_grads, weak_error, weak_diff_from_strong = get_ckpt_grads_and_weak_error(proj_grads, **weak_cfg)\n",
    "gt_ckpt_grads, _, _ = get_ckpt_grads_and_weak_error(proj_grads, **gt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa53f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an arrow plot where the color is weak_error\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap_name = \"coolwarm\"\n",
    "cmap = lambda x: plt.get_cmap(cmap_name)( (x - -1) / 2)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "weak_proj = pca.fit_transform(weak_ckpt_grads.cpu().numpy())\n",
    "gt_proj = pca.transform(gt_ckpt_grads.cpu().numpy())\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(len(weak_proj)):\n",
    "    plt.plot([gt_proj[i, 0], weak_proj[i, 0]], [gt_proj[i, 1], weak_proj[i, 1]], color=cmap(weak_error[i]))\n",
    "\n",
    "\n",
    "plt.scatter(gt_proj[:, 0], gt_proj[:, 1], c=weak_error, cmap=cmap_name, vmin=-1, vmax=1, marker='o', label=\"GT\")\n",
    "plt.scatter(weak_proj[:, 0], weak_proj[:, 1], c=weak_error, cmap=cmap_name, vmin=-1, vmax=1, marker='x', label=\"Weak\")\n",
    "\n",
    "deltas = (weak_proj - gt_proj)\n",
    "red_arrow = deltas[weak_error > 0.5].mean(axis=0)\n",
    "plt.arrow(0, 0, red_arrow[0], red_arrow[1], color='red', head_width=0.001, head_length=0.001, width=0.0001, label=\"Avg for Weak label > 0.5\", zorder=10)\n",
    "\n",
    "blue_arrow = deltas[weak_error < -0.5].mean(axis=0)\n",
    "plt.arrow(0, 0, blue_arrow[0], blue_arrow[1], color='blue', head_width=0.001, head_length=0.001, width=0.0001, label=\"Avg for Weak label < 0.5\", zorder=10)\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(f\"Gradient difference between weak and GT models\\n{cfg_str(weak_cfg)}\")\n",
    "plt.colorbar(label=\"Weak error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ed710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: take the empirical NTK, and project the gradients onto the tangent space of the model checkpoint. This should give us\n",
    "# a local parameter function map (that we should empirically test) that seems it must necessarily have the homogeneity hypothesis hold for\n",
    "# then if we do finetuning in the tangent space maybe we could find the homogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f1849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
