{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_path = \"../results/grads/sciq/bs=32-dl=1-dn=sciq-gib=1-ge=6-gee=10000000-lp=0-lbmae=0-l=xent-l=1e-07-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Mistral-7B-v0.1-ntd=2000-ntd=3000-ntd=6000-o=adam-stl=50-s=0-twd=0\"\n",
    "w2s_path = \"../results/grads/sciq/bs=32-dl=1-dn=sciq-gib=1-lp=0-lbmae=0-l=kl-l=5e-08-ls=cosi_anne-mc=512-mfbm=auro_agai_supe-ms=Mistral-7B-v0.1-ntd=2000-ntd=3000-ntd=6000-o=adam-stl=50-s=0-twd=0-we=3-wee=25-wlf=0.5-wms=Qwen1.5-0.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/alexm/miniconda3/envs/w2s/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1976 1976\n",
      "weak_hard_label acc tensor(0.7470)\n",
      "strong_hard_pred acc tensor(0.8897)\n",
      "w2s pred acc tensor(0.8406)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "\n",
    "strong_eval_results_path = os.path.join(strong_path, \"eval_results_final\")\n",
    "final_eval_results_path = os.path.join(w2s_path, \"eval_results_final\")\n",
    "final_eval_results = load_from_disk(final_eval_results_path)\n",
    "strong_eval_results = load_from_disk(strong_eval_results_path)\n",
    "print(len(final_eval_results), len(strong_eval_results))\n",
    "strong_eval_results = strong_eval_results.select(range(len(final_eval_results)))\n",
    "assert np.all(np.array(final_eval_results['id']) == np.array(strong_eval_results['id']))\n",
    "final_eval_results = final_eval_results.add_column(\"strong_soft_pred\", strong_eval_results['soft_pred'])  # type: ignore\n",
    "final_eval_results = final_eval_results.add_column(\"strong_hard_pred\", strong_eval_results['hard_pred'])  # type: ignore\n",
    "final_eval_results = final_eval_results.with_format(\"torch\")\n",
    "\n",
    "print(\"weak_hard_label acc\", ((final_eval_results['weak_soft_label'][:, 1] > 0.5) == final_eval_results['hard_label']).float().mean())\n",
    "print(\"strong_hard_pred acc\", (final_eval_results['strong_hard_pred'] == final_eval_results['hard_label']).float().mean())\n",
    "print(\"w2s pred acc\", (final_eval_results['hard_pred'] == final_eval_results['hard_label']).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "from weak_to_strong.config import ModelConfig, MODELS_DICT, LOSS_DICT\n",
    "from weak_to_strong.model import TransformerWithHead\n",
    "from weak_to_strong.common import to_batch\n",
    "from weak_to_strong.train import maybe_load_model\n",
    "\n",
    "config = json.load(open(os.path.join(w2s_path, \"config.json\"), \"r\"))\n",
    "model_name = config[\"model_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_lm_head = \"choice_input_ids\" in final_eval_results.column_names\n",
    "loss_fn = LOSS_DICT[config[\"loss\"]]\n",
    "\n",
    "d_proj = 10_000\n",
    "proj_grads, hiddens = defaultdict(lambda: defaultdict(dict)), defaultdict(lambda: defaultdict(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to remove LM head but it wasn't found.\n",
      "Hash(projection indices): 35579970425670\n",
      "Computing gradients for weak_soft_label in w2s\n",
      "Loading model from checkpoint_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1976it [14:06,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint_550.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:22,  2.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m         start_i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pg\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m     71\u001b[0m         pg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(grad_iter)\n\u001b[0;32m---> 72\u001b[0m     proj_grads[run_type][checkpoint][target_label_column][i, proj_i] \u001b[38;5;241m=\u001b[39m pg\u001b[38;5;241m.\u001b[39mflatten()[grad_idxr \u001b[38;5;241m-\u001b[39m start_i]\n\u001b[1;32m     74\u001b[0m hiddens[run_type][checkpoint][target_label_column][i, :] \u001b[38;5;241m=\u001b[39m hs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# zero out grads\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda:6\"\n",
    "grads_device = \"cuda:7\"\n",
    "n_eval = len(final_eval_results) # 100\n",
    "proj_grads_path = os.path.join(w2s_path, \"proj_grads.pt\")\n",
    "hiddens_path = os.path.join(w2s_path, \"hiddens.pt\")\n",
    "if os.path.exists(proj_grads_path):\n",
    "    proj_grads = torch.load(proj_grads_path)\n",
    "    hiddens = torch.load(hiddens_path)\n",
    "    print(\"Loaded existing proj_grads and hiddens\")\n",
    "else:\n",
    "    for run_type, results_path in [(\"w2s\", w2s_path), (\"strong\", strong_path),]:\n",
    "\n",
    "        # init model\n",
    "        mcfg = MODELS_DICT[model_name].copy()\n",
    "        if config[\"disable_lora\"]:\n",
    "            mcfg[\"lora_modules\"] = None\n",
    "        model_config = ModelConfig(**mcfg)\n",
    "        model = TransformerWithHead.from_pretrained(  # type: ignore\n",
    "                    model_config.name,\n",
    "                    lora_modules=model_config.lora_modules,\n",
    "                    use_lm_head=use_lm_head,\n",
    "                    num_labels=2,\n",
    "                    linear_probe=config[\"linear_probe\"],\n",
    "                    **model_config.custom_kwargs,\n",
    "                )\n",
    "        # # TODO: remove\n",
    "        # torch.nn.init.normal_(model.score.weight, std=1/4096**0.5, generator=torch.Generator().manual_seed(1))\n",
    "        # # TODO: delete model.lm.lm_head from learned head models before saving state dict\n",
    "        model_n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        torch.manual_seed(0)  # ensure that our projection is the same across runs and examples\n",
    "        proj_basis_indices = torch.randint(0, model_n_params, (d_proj,))  \n",
    "        proj_basis_indices, _ = proj_basis_indices.sort()\n",
    "        \n",
    "        print(f\"Hash(projection indices): {proj_basis_indices.sum().item()}\")\n",
    "        \n",
    "        for target_label_column in [\"weak_soft_label\", \"soft_label\"]:\n",
    "            print(f\"Computing gradients for {target_label_column} in {run_type}\")\n",
    "            for checkpoint in os.listdir(results_path):\n",
    "                if not checkpoint.startswith(\"checkpoint\") or run_type in proj_grads and checkpoint in proj_grads[run_type] and target_label_column in proj_grads[run_type][checkpoint]:\n",
    "                    continue\n",
    "                print(f\"Loading model from {checkpoint}\")\n",
    "                proj_grads[run_type][checkpoint][target_label_column] = torch.zeros((len(final_eval_results), d_proj), device=grads_device)\n",
    "                hiddens[run_type][checkpoint][target_label_column] = torch.zeros((len(final_eval_results), model.config.hidden_size), device=\"cpu\")\n",
    "\n",
    "                # load model checkpoint\n",
    "                assert maybe_load_model(model, os.path.join(results_path, checkpoint))\n",
    "                model.eval().to(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32).to(device)\n",
    "\n",
    "                for i, batch in tqdm(enumerate(to_batch(final_eval_results.select(range(n_eval)), batch_size=1))):\n",
    "                    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "                                [ex for ex in batch[\"input_ids\"]], batch_first=True\n",
    "                        ).to(model.device)\n",
    "                    label = batch[target_label_column].to(model.device)\n",
    "                    choice_ids = batch.get(\"choice_input_ids\")\n",
    "                    logits, hs = model(\n",
    "                                    input_ids, choice_input_ids=choice_ids.to(model.device) if choice_ids is not None else None, output_hidden_states=True\n",
    "                        )\n",
    "                        \n",
    "                    loss = loss_fn(logits, label, step_frac=0)\n",
    "                    loss.backward()\n",
    "\n",
    "\n",
    "                    # this mess avoids concatenating all the grads into one tensor before projecting, to save memory\n",
    "                    grad_iter = iter(p.grad for p in model.parameters() if p.grad is not None)\n",
    "                    pg = next(grad_iter)\n",
    "                    start_i = 0  # index into grad of the first component of pg\n",
    "                    for proj_i, grad_idxr in enumerate(proj_basis_indices):  # iterate over sorted projection indices\n",
    "                        while start_i + pg.numel() <= grad_idxr:  # while the current param is earlier than the desired index\n",
    "                            start_i += pg.numel()\n",
    "                            pg = next(grad_iter)\n",
    "                        proj_grads[run_type][checkpoint][target_label_column][i, proj_i] = pg.flatten()[grad_idxr - start_i]\n",
    "\n",
    "                    hiddens[run_type][checkpoint][target_label_column][i, :] = hs[-1][0, -1, :].detach().clone().cpu()\n",
    "\n",
    "                    # zero out grads\n",
    "                    model.zero_grad()\n",
    "    \n",
    "    def to_dict(defdict):\n",
    "        return {k: to_dict(v) if isinstance(v, defaultdict) else v for k, v in defdict.items()}\n",
    "    torch.save(to_dict(proj_grads), proj_grads_path)\n",
    "    torch.save(to_dict(hiddens),hiddens_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
