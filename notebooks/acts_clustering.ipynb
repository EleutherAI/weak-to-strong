{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.results import load_run_result, find_run_paths, RunResult\n",
    "from notebooks.analysis import erase_labels\n",
    "\n",
    "root = \"/mnt/ssd-1/alexm/weak-to-strong/results/logconf/amazon_polarity\"\n",
    "\n",
    "paths = find_run_paths(root)\n",
    "\n",
    "r_all = load_run_result(**paths)\n",
    "print(f\"Weak floor: {r_all.weak_acc:.2f}\")\n",
    "print(f\"Strong ceiling: {r_all.strong_acc:.2f}\")\n",
    "print(f\"W2S: {r_all.w2s_acc:.2f} (PGR: {r_all.pgr:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a range of training steps to keep data from\n",
    "r = r_all.select(0, r_all.run.n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def project_onto(x, y=None, method=\"lr\", pca=None, coef=None, **args):\n",
    "    if method == \"lr\":\n",
    "        assert y is not None\n",
    "        assert ((y == 0) | (y == 1)).all()\n",
    "        lr = LogisticRegression(**args)\n",
    "        lr.fit(x, y)\n",
    "        lr.coef_ /= np.linalg.norm(lr.coef_)\n",
    "        return x @ lr.coef_.flatten()\n",
    "    elif method == \"mean-diff\":\n",
    "        assert y is not None\n",
    "        assert ((y == 0) | (y == 1)).all()\n",
    "        classifier = x[y == 1].mean(axis=0) - x[y == 0].mean(axis=0)\n",
    "        classifier /= np.linalg.norm(classifier)\n",
    "        return x @ classifier\n",
    "    elif method.startswith(\"pca\"):\n",
    "        idx = int(method[len(\"pca\"):]) - 1\n",
    "        if pca is None:\n",
    "            pca = PCA(n_components=100)\n",
    "            pca.fit(x)\n",
    "        return pca.transform(x)[:, idx]\n",
    "    elif method == \"provided\":\n",
    "        assert coef is not None\n",
    "        return x @ coef\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method {method}\")\n",
    "    \n",
    "def get_labs(r: RunResult, name):\n",
    "    if name == \"is_correct\":\n",
    "        return r.w2s_train.gt_hard_labels == (r.w2s_train.weak_soft_labels > 0.5)\n",
    "    elif name == \"is_fp\":\n",
    "        return (r.w2s_train.gt_hard_labels == 0) & (r.w2s_train.weak_soft_labels > 0.5)\n",
    "    elif name == \"is_fn\":\n",
    "        return (r.w2s_train.gt_hard_labels == 1) & (r.w2s_train.weak_soft_labels < 0.5)\n",
    "    elif name == \"is_tp\":\n",
    "        return (r.w2s_train.gt_hard_labels == 1) & (r.w2s_train.weak_soft_labels > 0.5)\n",
    "    elif name == \"is_tn\":\n",
    "        return (r.w2s_train.gt_hard_labels == 0) & (r.w2s_train.weak_soft_labels < 0.5)\n",
    "    elif name == \"weak_hard_labels\":\n",
    "        return r.w2s_train.weak_soft_labels > 0.5\n",
    "    else:\n",
    "        return getattr(r.w2s_train, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "layer = 23\n",
    "h = r.w2s_train.pre_hiddens[:, layer, :]  # type: ignore\n",
    "# h, mask = erase_labels(h, r.run.weak_soft_labels, label_erasure=\"leace\")\n",
    "h = h - h.mean(axis=0)\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(h)\n",
    "\n",
    "l2 = 1e2\n",
    "args = {\"C\": 1 / l2, \"max_iter\": 1000}\n",
    "x_axis_method, x_axis = \"lr\", \"weak_hard_labels\"\n",
    "y_axis_method, y_axis = \"lr\", \"gt_hard_labels\"\n",
    "x_axis_labs = get_labs(r, x_axis)\n",
    "y_axis_labs = get_labs(r, y_axis)\n",
    "proj0 = project_onto(h, x_axis_labs, method=x_axis_method, pca=pca, **args)\n",
    "proj1 = project_onto(h, y_axis_labs, method=y_axis_method, pca=pca, **args)\n",
    "h_proj = np.stack([proj0, proj1], axis=1)\n",
    "\n",
    "ova_var = np.square(h).sum()\n",
    "exp_var1 = np.square(h_proj[:, 0]).sum() / ova_var\n",
    "exp_var2 = np.square(h_proj[:, 1]).sum() / ova_var\n",
    "auc1 = roc_auc_score(x_axis_labs > 0.5, h_proj[:, 0])\n",
    "auc2 = roc_auc_score(y_axis_labs > 0.5, h_proj[:, 1])\n",
    "print(f\"PCA Explained variance: {pca.explained_variance_ratio_.sum()}\")\n",
    "c = np.empty(r.w2s_train.n, dtype=float)\n",
    "c[get_labs(r, \"is_tn\")] = 0  # True negative\n",
    "c[get_labs(r, \"is_tp\")] = 1  # True positive\n",
    "c[get_labs(r, \"is_fn\")] = 0.25  # False negative\n",
    "c[get_labs(r, \"is_fp\")] = 0.75  # False positive\n",
    "for color, name in zip([0, 1, 0.25, 0.75], [\"TN\", \"TP\", \"FN\", \"FP\"]):\n",
    "    m = c == color\n",
    "    plt.scatter(h_proj[:, 0][m], h_proj[:, 1][m], c=c[m], cmap=\"coolwarm_r\", alpha=0.5, vmin=0, vmax=1, label=name)\n",
    "\n",
    "plt.xlabel(f\"{x_axis} {x_axis_method} (exp_var={100*exp_var1:.2f}%, auc={auc1:.2f})\")\n",
    "plt.ylabel(f\"{y_axis} {y_axis_method} (exp_var={100*exp_var2:.2f}%, auc={auc2:.2f})\")\n",
    "\n",
    "ds_name = r.cfg[\"ds_name\"]\n",
    "plt.title(f\"\\nds={ds_name}, lyr={layer}, L2_pen={l2}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "pca_h = pca.transform(h)[:, :20]\n",
    "pca_h, _ = erase_labels(pca_h, r.run.weak_soft_labels, label_erasure=\"leace\")\n",
    "\n",
    "clusterer = HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\n",
    "    gen_min_span_tree=False, leaf_size=40,\n",
    "    metric='euclidean', min_cluster_size=5, min_samples=None, p=None)\n",
    "clusterer.fit(pca_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "clusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette('deep', 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance = 1/lambd\n",
    "clusters = clusterer.single_linkage_tree_.get_clusters(1/0.21, min_cluster_size=100)\n",
    "plt.hist(r.w2s_train.gt_soft_labels[clusters == -1], density=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x; theta, b) = x @ theta\n",
    "# grad |f - y| = sign(y - f) * x\n",
    "# grad f = x\n",
    "# kernel grad = x_test @ sign(y - f) * x_train\n",
    "n_test = 1000\n",
    "test_idxs = np.random.choice(r.run.n, n_test, replace=False)\n",
    "layer = 20\n",
    "h = r.run.midtrain_hiddens[:, layer, :]\n",
    "test_h = h[test_idxs]\n",
    "\n",
    "train_grads = (2 * r.run.weak_soft_labels[:, None] - 1) * h\n",
    "kernel_grads = train_grads @ test_h.T\n",
    "\n",
    "# do batch averaging or something to eliminate unwanted variance\n",
    "batch_size, n_batches = 32, 1000\n",
    "batches = np.random.choice(len(h), n_batches * batch_size, replace=True)\n",
    "\n",
    "train_grads = (2 * r.run.weak_soft_labels[:, None] - 1) * h\n",
    "batch_grads = train_grads[batches].reshape(n_batches, batch_size, -1).mean(axis=1)\n",
    "batch_kernel_grads = batch_grads @ test_h.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kernel_grads[:, 0], kernel_grads[:, 1], alpha=0.1, c=get_labs(r, \"is_correct\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
